---
title: "Lecture: R and Frequentist Inference"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    css: css/learnr-theme.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
## --- learnr ---
if ("learnr" %in% (.packages()))
  detach(package:learnr, unload = TRUE)
library(learnr)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

## ---- CRAN Packages ----
## Save package names as a vector of strings
pkgs <-  c("dplyr", "MASS", "ggplot2", "ggpubr")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

## ---- GitHub Packages ----
devtools::install_github("denis-cohen/regplane3D")
library(regplane3D)

## ---- Global learnr Objects ----
x <- 9     # a real number
y <- 4L    # an integer

start <- "R is so much"
end <- "fun!"
sentence <- paste(start, end, sep = " ")

item <- c("flour", "sugar", "potatoes", "tomatoes", "bananas")
price <- c(.99, 1.49, 1.99, 2.79, 1.89)
pricey <- price > 1.5

mat1 <- matrix(seq(-.35, .35, .1), nrow = 2, ncol = 4)
mat2 <- rbind(item, substr(item, start = 1, stop = 3))
mat3 <- cbind(price > 1.5, price < 1)

a <- c(0.5, 1, 1.5)
b <- c(1, 2, 3)
s <- t(a) %*% b
m <- matrix(1:4, nrow = 2, ncol = 2)

K <- 3L
L <- 2L
M <- 2L
array3D <- array(rnorm(K*L*M, mean = 0, sd = 1), dim = c(K, L, M))

groceries <- data.frame(
  item = c("flour", "sugar", "potatoes", "tomatoes", "bananas"),
  price = c(.99, 1.49, 1.99, 2.79, 1.89)
)

my_list <- list()
my_list$char_mat <- matrix(LETTERS, nrow = 2, ncol = 13)
my_list$vectors <- list()
my_list$vectors$num_vec1 <- c(1:2)
my_list$vectors$num_vec2 <- runif(10, min = 0, max = 1)
my_list$isLame <- TRUE
```

## R and frequentist inference

### Structure of this session

#### Very quick refreshers

1.  Data types
2.  Arithmetic operators and transformations
3.  Object types
4.  Object indexing

#### Moderately quick refreshers

1.  For loops
1.  Apply functions

#### Not-so-quick refreshers

1.  Probability distributions
2.  Frequentist statistical inference

## Data types

```{r num_1, echo=TRUE, eval = FALSE}
# Real numbers ("double")
x <- 9
z <- 13.2

# Integers
y <- 4L

# Characters
start <- "R is so much"
end <- "fun!"
sentence <- paste(start, end, sep = " ")

# Logicals & Boolean operators
2 + 5 == 7
3 %in% c(1:5) & 7 %in% c(1:5)
```

## Arithmetic operators and transformations

```{r ari, eval=FALSE, echo=TRUE}
x + y     # addition
x - y     # subtraction
x * y     # multiplication
x / y     # division
x ^ y     # exponentiation

log(x)    # natural logarithm
exp(x)    # exponential
sqrt(x)   # square root
```

## Object types

### Vectors

A vector is a serial listing of data elements of the same type (e.g. `integer`, `double`, `logical`, or `character`).

```{r vec1_1, echo = TRUE}
item <- c("flour", "sugar", "potatoes", "tomatoes", "bananas")
price <- c(.99, 1.49, 1.99, 2.79, 1.89)
pricey <- price > 1.5
```

### Matrices

A matrix is a rectangular arrangement of data elements of the same type.

```{r mat1_1, echo = TRUE}
mat1 <- matrix(seq(-.35, .35, .1), nrow = 2, ncol = 4)
mat1
```

### Arrays

An array is a multidimensional arrangement of data elements of the same type.

You may think of an array as a generalization of a matrix to any number of dimensions:

-   all elements are of the same type
-   fixed dimensional structure
-   any vector or matrix can be stored as an array using `as.array()`

### Lists

Lists allow you to store objects of various classes, various storage types, and various sizes. This can be very useful, e.g., for returning various outputs from a function.

```{r list1_1 , echo = TRUE}
my_list <- list()
my_list$char_mat <- matrix(LETTERS, nrow = 2, ncol = 13)
my_list$vectors <- list()
my_list$vectors$num_vec1 <- c(1:2)
my_list$vectors$num_vec2 <- runif(10, min = 0, max = 1)
```

### Data Frames

Data frames are lists of variables of equal length, producing a rectangular structure. Unlike matrix columns, data frame columns can be of different storage types and classes.

```{r df, echo = TRUE}
groceries <- data.frame(
  item = c("flour", "sugar", "potatoes", "tomatoes", "bananas"),
  price = c(.99, 1.49, 1.99, 2.79, 1.89)
)

groceries
```

Tibbles are data frames with an added class. They (arguably) show more intuitive behavior than data frames.

## Object indexing

### Numerical indexing

We can slice vectors, matrices, or arrays using *numerical* indexing.

```{r slice-1, exercise = TRUE, echo = TRUE}
mat1
mat1[1, 3:4]
```

### Indexing by names

When objects contain attributes such as `names()` (`list`, `data.frame`), `rownames()` and `colnames()` (`matrix`), or `dimnames()` (`array`), we can also index by character names:

### Naming and slicing a vector

```{r slice-3, exercise = TRUE, echo = TRUE}
names(price) <- item
str(price)
price[c("potatoes", "bananas")]
```

### Logical indexing

Lastly, we can use logical evaluations to select subsets of elements. Suppose we only want those objects from groceries that are countable (i.e., end with an "s") and cost at least EUR 2.00.

```{r slice-7, exercise = TRUE, echo = TRUE}
# Countable items
groceries$item
is_countable <- 
  substr(groceries$item, nchar(groceries$item), nchar(groceries$item)) == "s"

# Pricey items
groceries$price
is_very_pricey <- groceries$price > 2.00

# Find subset
groceries[is_very_pricey & is_countable, ]
```

## Loops and apply functions

### Auxiliary tools for indexing

`seq_len()` and `seq_along()` are very helpful in defining integer sequences:

```{r seq, exercise = TRUE, echo = TRUE}
# seq_len()
nrow(groceries)
1:nrow(groceries)
seq_len(nrow(groceries))

# seq_along
length(item)
seq_along(item)
```

### for-loop

```{r for, exercise = TRUE, echo = TRUE}
for (i in seq_len(nrow(groceries))) {
  print(paste0("Item ", i, ": ", groceries$item[i]))
}
```

### if, else

Let's re-establish which foods are pricey ($\geq 1.50\text{EUR})$ ...

```{r if-1, exercise = TRUE, echo = TRUE}
for (i in seq_len(nrow(groceries))) {
  if (groceries$price[i] >= 1.5) {
    cat(paste0("Item ", i, " (", groceries$item[i], "): Sooooo pricey!", "\n"))
  } else {
    cat(paste0("Item ", i, " (", groceries$item[i], "): Sooooo cheap!", "\n"))
  }
}
```

We may want to store this information in an object (e.g., a vector or a variable).

*As a general rule:*

-   Whenever possible, initialize an empty *container* of fixed dimensions and subsequently fill it with values.
-   Avoid sequentially growing objects by appending elements/rows/columns via `c()`, `rbind()`, or `cbind()`.

```{r if-2, exercise = TRUE, echo = TRUE}
## Initialize container (here, a vector)
pricey_or_not <- rep(NA, nrow(groceries))

## Fill container
for (i in seq_len(nrow(groceries))) {
  if (groceries$price[i] >= 1.5) {
    pricey_or_not[i] <- "pricey"
  } else {
    pricey_or_not[i] <- "cheap"
  } 
}
```

### Nested loops

Nested loops involve an inner loop within an outer loop. Operations within the inner loop are usually conditional on values defined by the outer loop.

An example: Suppose a wholesaler offers our grocery items in bulk. A single unit is shipped at the full price, 50 units are shipped at a discount of 10%, and 100 units are shipped at a 20% discount.

```{r nested, echo=TRUE}
discounts <- data.frame(units = c(1, 50, 100),
                        discount = c(0, 0.1, 0.2))

for (i in seq_len(nrow(groceries))) {
  # Print item number and name
  cat(paste0("Item ",
             i,
             " (",
             groceries$item[i],
             ")\n"))
  for (j in seq_len(nrow(discounts))) {
    # Print bulk price and price per unit for each item
    cat(
      paste0(
        discounts$units[j],
        " unit(s) cost EUR ",
        groceries$price[i] *
          discounts$units[j] *
          (1 - discounts$discount[j])
        ,
        " (EUR ",
        round(groceries$price[i] *
                (1 - discounts$discount[j]), 2),
        " per unit)\n"
      )
    )
  }
  # Draw horizontal separator
  cat("----------------------------------------------\n")
}
```

### apply

Use `apply()` on matrices or arrays to apply a function across selected dimension(s) of a matrix or array.

### Row- and column-means

```{r apply-1, exercise = TRUE, echo = TRUE}
mat1
apply(mat1, 1, mean)
apply(mat1, 2, mean)
```

### Custom functions

You can apply custom functions to all apply functions:

```{r apply-2, exercise = TRUE, echo = TRUE}
count_negative <- function (x) {
  sum(x < 0)
}
apply(mat1, 1, count_negative)
apply(mat1, 2, count_negative)
```

## Probability distributions

### Univariate probability distributions

Univariate probability distributions give the distribution of probabilities for all feasible realizations of a single *random variable*.

Feasible means that it characterizes the (cumulative) probability mass of each possible outcome for a *discrete/categorical* variables or the (cumulative) probability density of each value within the support of a *continuous* variable.

### Functions

For common probability distributions, R features the following four commands (where `dist` is a placeholder):

1.  `ddist(x)`: Probability density/mass function (pdf/pmf). Takes a value $x$ and returns the probability density/mass $P(X=x)$.\
2.  `pdist(x)`: Cumulative distribution function (CDF). Takes a value $x$ and returns the cumulative probability $P(X \leq x)$.
3.  `qdist(q)`: Quantile function or inverse CDF. Takes a cumulative probability $P(X \leq x)$ and returns the corresponding value $x$.
4.  `rdist(n)`: Produces $n$ random draws from the distribution.

### Discrete case: The binomial distribution

A binomial distribution characterizes the probability of outcomes for a series of `n` independent realization of a binary random variable.

Numerically, we define a binary variable as a variable that can either take on a value of zero ("failure") or one ("success").

An example is a series of coin flips $n$, each of which produces either heads or tails. The numerical assignment of ones and zeroes is arbitrary. Here, we will consider heads as ones or successes. We denote the number as heads/successes as $k$.

The binomial distribution is governed by a single *probability parameter*, $\pi \in [0,1]$. It determines the probability of successes/ones for each realization. Accordingly, $1 - \pi$ gives the probability of failure/zeroes.

In our example, $\pi$ determines the fairness of a coin. A fair coin with $\pi = 0.5$ is equally likely to produce heads and tails for each flip.

### Probability mass function (pmf)

Below, we show the probability mass function of a random variable $X$ that records the probability of each count of heads $k \in \{0, 1, 2, 3, 4\}$ out of $n=4$ flips with a fair coin, i.e., $\pi = 0.5$: $k \sim \text{Binom}(4, 0.5)$.

```{r binom-1, fig.align = "center", include = TRUE}
n <- 4L
x <- 0:n
pmf <- dbinom(x = x,
              size = n,
              prob = 0.5)
plot(
  x,
  pmf,
  type = 'p',
  pch = 19,
  xlab = "k",
  ylab = "P(X=k)",
  main = "Binomial(4, 0.5)",
  ylim = c(0, max(pmf)),
  xlim = range(x) + c(-.5, .5)
)
for (i in seq_along(x)) {
  segments(x0 = x[i],
           y0 = 0,
           x1 = x[i],
           y1 = pmf[i])
}
```

What do these probabilities sum to?

### Cumulative distribution function (CDF)

```{r binom-2, fig.align = "center", include = TRUE, fig.height=8, fig.width=7}
n <- 4L
x <- 0:n
pmf <- dbinom(x = x,
              size = n,
              prob = 0.5)
cdf <- pbinom(q = x,
              size = n,
              prob = 0.5)

# Initialize plot
par(mfrow = c(2, 1))

## pmf
plot(
  x,
  pmf,
  type = 'p',
  pch = 19,
  xlab = "k",
  ylab = "P(X=k)",
  main = "Binomial(4, 0.5) pmf",
  ylim = c(0, max(pmf)),
  xlim = range(x) + c(-.5, .5)
)
for (i in seq_along(x)) {
  segments(x0 = x[i],
           y0 = 0,
           x1 = x[i],
           y1 = pmf[i])
}

## CDF
plot(
  x,
  cdf,
  type = 'p',
  pch = 19,
  xlab = "k",
  ylab = expression("P(X"<="k)"),
  main = "Binomial(4, 0.5) CDF",
  ylim = c(0, 1),
  xlim = range(x) + c(-.5, .5)
)
for (i in seq_along(x)) {
  segments(x0 = x[i],
           y0 = 0,
           x1 = x[i],
           y1 = cdf[i])
}
```

### Continuous case: The standard normal distribution

Let's illustrate the continuous case using everyone's favorite, the standard normal.

The normal distribution is characterized by two parameters: Mean $\mu$ and *standard deviation* $\sigma$.

The standard normal distribution is a special case where $\mu = 0$ and $\sigma = 1$, $\text{N} \sim (0, 1)$:

```{r stdnorm-1, fig.align = "center", include = TRUE}
x <- seq(-3, 3, .01)
y <- dnorm(x)
plot(
  x,
  y,
  type = 'l',
  xlab = "x",
  ylab = "P(X=x)",
  main = "Standard normal pdf"
)
polygon(
  c(x, rev(x)),
  c(rep(0, length(y)), rev(y)),
  col = adjustcolor("gray50", alpha.f = 0.25),
  border = NA
)
```

### pdf

To get the probability density at any value $x$, e.g., $x = -1$, run:

```{r stdnorm-2, exercise = TRUE, echo = TRUE}
dnorm(-1, mean = 0, sd = 1)
```

```{r stdnorm-3, fig.align = "center", include = TRUE}
x <- seq(-3, 3, .01)
y <- dnorm(x)
x_star <- -1
y_star <- dnorm(x_star)
plot(
  x,
  y,
  type = 'l',
  xlab = "x",
  ylab = "P(X=x)",
  main = "Standard normal pdf"
)
polygon(
  c(x, rev(x)),
  c(rep(0, length(y)), rev(y)),
  col = adjustcolor("gray50", alpha.f = 0.25),
  border = NA
)
segments(x_star, -3.5, x_star, y_star, lty = 3)
segments(-3.5, y_star, x_star, y_star, lty = 3)
```

### CDF and quantile function

To get the cumulative probability up to any value $x$, e.g., $x = 1.9599$, run:

```{r stdnorm-4, exercise = TRUE, echo = TRUE}
pnorm(1.9599, mean = 0, sd = 1)
```

```{r stdnorm-5, fig.align = "center", include = TRUE, fig.height = 8, fig.width=7}
x <- seq(-3, 3, .01)
y <- dnorm(x)
cdf <- pnorm(x)
x_star <- qnorm(.975)

# Initialize plot
par(mfrow = c(2, 1))

# pdf
plot(
  x,
  y,
  type = 'l',
  xlab = "x",
  ylab = "P(X=x)",
  main = "Standard normal pdf"
)
polygon(
  c(x[x <= x_star], rev(x[x <= x_star])),
  c(rep(0, length(y[x <= x_star])), rev(y[x <= x_star])),
  col = adjustcolor("gray50", alpha.f = 0.25),
  border = NA
)

# CDF
plot(
  x,
  cdf,
  type = 'l',
  xlab = "x",
  ylab = expression("P(X"<="x)"),
  main = "Standard normal CDF",
  ylim = c(0, 1)
)
segments(
  x0 = -3.5,
  x1 = x_star,
  y0 = .975,
  y1 = .975,
  lty = 3
)
segments(
  x0 = x_star,
  x1 = x_star,
  y0 = 0,
  y1 = .975,
  lty = 3
)
```

Conversely, use `qnorm` to get the $x$ values for any desired cumulative probability:

```{r stdnorm-6, exercise = TRUE, echo = TRUE}
qnorm(c(.025, .5, .975))
```

```{r stdnorm-7, fig.align = "center", include = TRUE}
x <- seq(-3, 3, .01)
y <- dnorm(x)
x_star <- qnorm(c(.025, .5, .975))
y_star <- dnorm(x_star)

par(mfrow = c(1, 1))
plot(
  x,
  y,
  type = 'l',
  xlab = "x",
  ylab = "P(X=x)",
  main = "Standard normal pdf"
)
for (i in rev(seq_along(x_star))) {
  polygon(
    c(x[x <= x_star[i]], rev(x[x <= x_star[i]])),
    c(rep(0, length(y[x <= x_star[i]])), rev(y[x <= x_star[i]])),
    col = adjustcolor("gray50", alpha.f = 0.35),
    border = NA
  )
}
```

### Random number generation

Lastly, to generate random draws from a distribution with specific parameters, use `rnorm()`. The higher the number of draws, the better the chances that the frequency distribution of your random draws will approximate the underlying pdf:

```{r stdnorm-8, exercise = TRUE, echo = TRUE, fig.align='center'}
# Define number of draws
n_sim <- 1000000L

# Set a seed for replicability
set.seed(20231122L)

# Take random draws
x_sim <- rnorm(n_sim, mean = 0, sd = 1)

# Visualize
hist(
  x_sim,
  main = paste0("Simulated Standard Normal pdf, N = ", n_sim),
  xlab = "x",
  ylab = "Pr(X=x)",
  freq = FALSE,
  breaks = log(n_sim) * 3,
  border = "gray60"
)
```

*Note:* `set.seed()` ensures the replicability of random number generation (!).

### Multivariate probability distributions

Bivariate probability distributions give the distribution of probabilities for all feasible *joint* realizations of *two* random variables.

Multivariate probability distributions generalize this idea to *three or more* random variables.

Multivariate probability distributions carry the same information for the constitutive variables as univariate probability distributions. We call these variable-specific distributions *marginal probability distributions*.

Additionally, multivariate probability distributions carry information about the *mutual dependence* of the constitutive variables. This is typically captured by *covariance parameters*. Independent random variables share a covariance of zero.

### Example: The bivariate standard normal distribution (independent case)

The following shows the joint probability density function of a two standard normal distributions.

We can think of it as a bell-shaped hill with a circular base whose height reaches its peak at $X1 = 0$ and $X2 = 0$.

```{r bvnnorm-1, fig.align = "left", include = TRUE,, fig.width=11.67, fig.height=6.67}
x_axis <- regplane3D::pretty_axis_inputs(
  axis_range = c(-3, 3),
  base = 1,
  nlines_suggest = 21L,
  multiply = 5
)
y_axis <- regplane3D::pretty_axis_inputs(
  axis_range = c(-3, 3),
  base = 1,
  nlines_suggest = 21L,
  multiply = 5
)
z <- dnorm(x_axis$seq) %*% t(dnorm(y_axis$seq)) %>%
  replicate(3L, .) %>%
  simplify2array()

regplane3D::plane3D(
  z = z,
  x = x_axis$seq,
  y = y_axis$seq,
  zlim = c(0, 0.175),
  xlim = x_axis$range,
  ylim = y_axis$range,
  zlab = "Joint probability density",
  xlab = "X1",
  ylab = "X2",
  main = "Independent Bivariate\nStandard Normal Distribution",
  cis = TRUE,
  xnlines = x_axis$nlines,
  ynlines = y_axis$nlines,
  expand = 0.8,
  theta = 135,
  phi = 33
)
```

### Heatmaps

Heatmaps are a great way of visualizing joint densities in 2D.

They take a bird's eye perspective and re-express the third dimension (height) through the intensity of color shadings.

They allow us to add information on the marginal distributions, too.

```{r bvnnorm-2, fig.align = "center", include = TRUE, fig.width=7, fig.height=7}
heatmap_data <- expand.grid(
  x = x_axis$seq,
  y = y_axis$seq
) %>%
  dplyr::mutate(
    z = dnorm(x) * dnorm(y)
  )

# x1/x2 marginals
marginal_x1 <- marginal_x2 <- ggplot(data = data.frame(x = c(-100, 100)), aes(x)) +
  stat_function(fun = dnorm,
                n = 10001,
                args = list(mean = 0, sd = 1),
                geom = "polygon",
                color = "black",
                fill = "red",
                alpha = 0.25) + 
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL)  +
  xlim(-3, 3) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

# Joint density
joint_density <- heatmap_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_raster(aes(fill = z)) +
  scale_fill_viridis_c() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlab("X1") +
  ylab("X2") +
  theme(legend.position = "none") +
  xlim(-3, 3) +
  ylim(-3, 3) +
  theme(plot.margin = unit(c(0, 0, 1, 1), 'lines'))

## Multiplot
ggarrange(
  marginal_x1,
  NULL,
  joint_density,
  marginal_x2 +
    rotate(),
  nrow = 2,
  ncol = 2,
  align = "hv",
  widths = c(2, 1),
  heights = c(1, 2)
)
```

### Example: The bivariate normal distribution (interdependent case)

```{r bvnnorm-3, fig.align = "center", include = TRUE, fig.width=7, fig.height=7}
# BVN pdf
dbvnnorm <- function(x, y, mu_x, sd_x, mu_y, sd_y, rho) {
  return(
    (1 / (2 * pi * sd_x * sd_y * sqrt(1 - rho ^ 2))) *
      exp(-(
        (1 / (2 * (1 - rho ^ 2))) * (
          ((x - mu_x) / sd_x) ^ 2 +
            ((y - mu_y) / sd_y) ^ 2 - 
            2 * rho * (
              (x - mu_x) * (y - mu_y) / (sd_x * sd_y)
            )
        )
      ))
  )
}

# Define parameters
mu_x = 0
sd_x = 1
mu_y = -5
sd_y = 2.5
rho = -0.5

# Data
heatmap_data <- expand.grid(x = seq(-3, 3, length.out = 101L) * sd_x + mu_x,
                            y = seq(-3, 3, length.out = 101L) * sd_y + mu_y) %>%
  dplyr::mutate(z = dbvnnorm(
    x,
    y,
    mu_x,
    sd_x,
    mu_y,
    sd_y,
    rho
  ))

# x1/x2 marginals
marginal_x1 <- ggplot(data = data.frame(x = c(-100, 100)), aes(x)) +
  stat_function(fun = dnorm,
                n = 10001,
                args = list(mean = mu_x, sd = sd_x),
                geom = "polygon",
                color = "black",
                fill = "red",
                alpha = 0.25) + 
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL)  +
  xlim(range(heatmap_data$x)) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

marginal_x2 <- ggplot(data = data.frame(x = c(-100, 100)), aes(x)) +
  stat_function(fun = dnorm,
                n = 10001,
                args = list(mean = mu_y, sd = sd_y),
                geom = "polygon",
                color = "black",
                fill = "red",
                alpha = 0.25) + 
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL)  +
  xlim(range(heatmap_data$y)) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

# Joint density
joint_density <- heatmap_data %>%
  ggplot(aes(x = x, y = y)) +
  geom_raster(aes(fill = z)) +
  scale_fill_viridis_c() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlab("X1") +
  ylab("X2") +
  theme(legend.position = "none") +
  xlim(range(heatmap_data$x)) +
  ylim(range(heatmap_data$y)) +
  theme(plot.margin = unit(c(0, 0, 1, 1), 'lines'))

## Multiplot
ggarrange(
  marginal_x1,
  NULL,
  joint_density,
  marginal_x2 +
    rotate(),
  nrow = 2,
  ncol = 2,
  align = "hv",
  widths = c(2, 1),
  heights = c(1, 2)
)
```

Can you guess:

-   The approximate means and standard deviations of $X_1$ and $X_2$?
-   Whether the correlation between X1 and X2 is positive or negative?

## Frequentist inference

### Ask Wikipedia?

The first two sentences of the [English-language Wikipedia article](https://en.wikipedia.org/wiki/Statistical_inference) defines statistical inference as follows:

<blockquote>
1.  Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability.
2.  Inferential statistical analysis **infers properties of a population**, for example by testing hypotheses and deriving estimates.
</blockquote>

The first sentence is universally true across both frequentist and Bayesian frameworks of statistical inference.

However, as we will learn in this course, the bold-printed is exclusive to the *frequentist* framework of statistical inference.

### Key characteristics

- Frequentist analysis seeks to infer *population parameters*. Such parameters -- e.g., the population mean of a variable $X$ -- are considered *fixed*: Their values are true, exact, and unknown.
- To learn about such population parameters, we rely on a finite sample. Every such sample is considered one out of many possible random samples from the underlying population.
- This implies *repeatable data*: In theory, we should have a constant stream or pool of independently and identically distributed data, from which we could sample as often as we like.
- Every random sample is an imperfect representation of the underlying population due to *sampling variability*.
- Imagine we took many independent samples from the same underlying population, and computed the sample analogue of the population parameter for each of them -- using a suitable estimator/statistic, the resulting distribution of sample statistics would yield a probability distribution known as the *sampling distribution*.

### Repeated sampling: A though experiment

- Consider an infinite population.
- We want to infer the population proportion of a binary variable $X$ -- say, the number of café-goers who order beverages with oat milk.
- For this task, we will simulate $S=10,000$ samples of size $N = 100$.
- Unbeknownst to us, the true population proportion is $\pi = 0.3$.
- Given that $X$ is binary, we treat each sample as a series of independent Bernoulli trials -- i.e., a draw from a $\text{Binomial}(100, 0.3)$ distribution, where the probability parameter is equal to the population proportion of $X$.

Let us first look at the variability in the number of oat-milk drinkers in our samples:

```{r sim-data, fig.align = "center", include = TRUE, fig.width=7, fig.height=7}
n_samples <- 10000L
n_obs <- 50L
set.seed(20231121L)
samples <- rbinom(n_samples, n_obs, prob = 0.3)

par(mfrow = c(1, 1))
plot(
  x = sort(samples),
  y = seq_along(samples),
  type = "p",
  pch = 19,
  cex = 0.1,
  xlim = c(0, 50),
  xlab = "Number of oat-milk drinkers per sample",
  ylab = "Samples (sorted)"
)
abline(v = mean(samples),
       col = adjustcolor("red", 0.5))
```

As we can see, there is quite some variability: While many (precisely, `r sum(samples %in% 10:20)`) of our 10,000 samples produce between 10 and 20 oat-milk drinkers (with a mean of roughly `r round(mean(samples), 2)`), the numbers go as low as `r min(samples)` and as high as `r max(samples)`.

This is indicative of the significant sampling variability we face when working with small samples.

### A simulated sampling distribution

We can re-express the raw sample counts by calculating the *sample proportion* within each sample. This is also known as an *estimator* or *statistic* -- a function that takes data as inputs to produce an aggregate output. The specific value this function produces is called an *estimate*.

Instead of plotting the number of the index of the sorted samples along the x-axis, we will now plot the **number of simulations** that produce a specific estimate.

```{r sampling-dist, fig.align = "center", include = TRUE, fig.width=7, fig.height=7}
sample_proportions <- table(samples / n_obs)

par(mfrow = c(1, 1))
plot(
  x = as.numeric(names(sample_proportions)),
  y = as.numeric(sample_proportions),
  type = "l",
  xlab = "Sample proportions of oat-milk drinkers",
  ylab = "Number of simulations"
)
abline(v = mean(samples) / n_obs,
       col = adjustcolor("red", 0.5))
```

The above is the **sampling distribution of the estimator** -- in this case, the sampling distribution of the sample proportion for a sample of size 50 from the same underlying distribution.

### Sampling distributions

You will recognize this distribution as *approximately normal*. This is due to the **central limit theorem**: With sufficiently large samples, the probability distribution of the sample mean (analogously, the sample proportion) across many such samples converges to a normal distribution.

Some facts about sampling distributions:

- Its mean is the true population mean.
- Its standard deviation gives the **standard error** of the estimator.
- Its 2.5 and 97.5 percentiles give the **95% confidence interval**. Since the sampling distribution is normal, the 95% confidence interval can be derived as $\text{mean} \pm 1.96 \times \text{std. err.}$.
- The true sampling distribution of an estimator is **never known**. It can only be approximated by actually taking many, many independent samples.

### The sampling distribution in practice

So, if the *true* sampling distribution is never known, and cannot be recovered from a single sample, what do we do?

We take the sample statistics of a singular sample at face value:

- Take the sample proportion as the best available estimate of the true population proportion: $\hat{p} = \frac{1}{N}\sum_{i=1}^{N} X_i$
- Take the standard error of the sample proportion as the best available estimate of the standard error of the estimator: $\hat{\sigma}_p = \sqrt{\frac{\hat{p} (1- \hat{p})}{N}}$
- Derive the 95% confidence interval accordingly (remember to use the correct $t$-value in place of the $z$-score!)

### Example

```{r sample-stats-1, include=FALSE}
p <- samples[1] / n_obs
se <- sqrt(((samples[1] / n_obs) * (1 - samples[1] / n_obs)) / n_obs)
ci <- p + qt(c(0.025, .975), df = n_obs - 1) * se
```

The first of our 10,000 samples produces the following number of oat-milk drinkers: `r samples[1]`:

- The sample proportion is `r round(p, 3)`.
- The standard error of the sample proportion is `r round(se, 3)`.
- The corresponding 95% confidence interval is `r paste(round(ci, 3), collapse = ", ")`.
- It `r ifelse(prod(ci - 0.3) > 0, "does not", "does")` contain the true population parameter.

What if we had ended up with our second sample instead?

```{r sample-stats-2, include=FALSE}
p <- samples[2] / n_obs
se <- sqrt(((samples[2] / n_obs) * (1 - samples[1] / n_obs)) / n_obs)
ci <- p + qt(c(0.025, .975), df = n_obs - 1) * se
```

- The number of oat-milk drinkers is `r samples[2]`:
- The sample proportion is `r round(p, 3)`.
- The standard error of the sample proportion is `r round(se, 3)`.
- The corresponding 95% confidence interval is `r paste(round(ci, 3), collapse = ", ")`.
- It `r ifelse(prod(ci - 0.3) > 0, "does not", "does")` contain the true population parameter.

### The correct interpretation of a 95% confidence interval

As we have just learned, whether a 95% confidence interval estimated from a given sample covers the true population parameter depends how (un)lucky we are with our sample.

A given 95% confidence interval does **not** contain the true parameter with 95% probability or "certainty". It either does or it doesn't, so the probability is either zero or one.

A 95% confidence interval [*"represents the long-run proportion of CIs (at the given confidence level) that theoretically contain the true value of the parameter"*](https://en.wikipedia.org/wiki/Confidence_interval).

We can see this by looking back at our 10,000 samples:

```{r sim-cis, fig.align = "center", include = TRUE, fig.width=7, fig.height=7}
p <- sort(samples / n_obs)
se <- sqrt((p * (1 - p) / n_obs))
ci_lo <- p + qt(.025, df = n_obs - 1) * se
ci_hi <- p + qt(.975, df = n_obs - 1) * se
contains_true <- ci_lo <= 0.3 & ci_hi >= 0.3

par(mfrow = c(1, 1))
plot(
  x = p,
  y = seq_along(p),
  type = "p",
  pch = 19,
  cex = 0.1,
  xlab = "Proportion of oat-milk drinkers per sample",
  ylab = "Samples (sorted)",
  xlim = c(0, 0.75)
)
abline(v = 0.3,
       col = "black")
segments(
  x0 = ci_lo,
  x1 = ci_hi,
  y0 = seq_along(p),
  y1 = seq_along(p),
  lwd = 0.001,
  col = adjustcolor(ifelse(contains_true, "black", "red"), alpha.f = .1)
)
points(
  x = p,
  y = seq_along(p),
  pch = 19,
  cex = .1,
  col = "white"
)
```

The proportion of 95% CIs that do contain the true mean is `r mean(contains_true)`.

### $p$-values and the logic of frequentist hypothesis testing

Similarly, $p$-values do *not* give the probability of support for an (alternative) hypothesis.

They give probability of finding other results at least as extreme as the present result, given a null hypothesis, as the long-run frequency across many samples.

Example: Let our hypothesis be that the proportion of oat-milk drinkers in the population is 0.5.

Remember our first sample had `r samples[1]` oat milk drinkers, a proportion of `r p[1]`.

To test our hypothesis based on this sample data, we first need a t-score:

```{r t-score, echo = TRUE}
t <- (p[1] - 0.5) / se[1]
t
```

We can the calculate the $p$-value for a two-tailed test as

```{r p-val, echo = TRUE}
2 * pt(-abs(t), df = n_obs - 1)
```

This value is near zero. This shows that we would expect that hardly any other sample would produce a result as extreme as ours if the null hypothesis were true. We would therefore *reject* the null hypothesis.
