---
title: "Lab: Bayesian and Frequentist Inference"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    css: css/learnr-theme.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
## --- learnr ---
if ("learnr" %in% (.packages()))
  detach(package:learnr, unload = TRUE)
library(learnr)
knitr::opts_chunk$set(echo = FALSE)

## ---- CRAN Packages ----
## Save package names as a vector of strings
pkgs <-  c("foreign", "MASS", "coda")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

## ---- GitHub Packages ----


## ---- Global learnr Objects ----
## Data
gles <- 
  read.dta("https://github.com/denis-cohen/statmodeling/raw/main/data/gles.dta")

y <- gles$sup_afd
X <- model.matrix( ~ se_self +
                     la_self +
                     se_self:la_self,
                   data = gles)

## Functions
lm_gibbs <- function(m_scalar,
                     p_scalar,
                     a,
                     b,
                     n_warmup,
                     n_draws,
                     y,
                     X,
                     seed = 20210329,
                     keep_warmup = TRUE) {
  # Set seed
  set.seed(seed)
  
  # Length of chain
  len_chain <- n_warmup + n_draws
  
  # Get vector m, matrix P (note: P = V^(-1))
  m <- rep(m_scalar, ncol(X))
  P <- diag(rep(p_scalar, ncol(X)))
  
  # Neither m_star nor V_star are conditioned on a_star, b_star, or tau
  # So we only need to define these once. As we also need the inverse of
  # V_star, we can store it in an object named P_star.
  V_star <- solve(t(X) %*% X + P)
  P_star <- solve(V_star)
  m_star <- as.vector(V_star %*% (t(X) %*% y + P %*% m))
  
  # Data characteristics
  n_data <- length(y)  

  # Initialize containers
  beta <- matrix(NA, len_chain, ncol(X))
  tau <- rep(NA, len_chain)
  
  # Note that neither 
  
  # Run Gibbs sampler
  for (i in seq_len(len_chain)) {
    if (i == 1) {
      ## Iteration 1: Initialize from prior
      a_star <- a
      b_star <- b
    } else {
      ## Iterations 2+: Update a and b
      a_star <- a + n_data / 2
      b_star <- as.vector(b +
                            0.5 * (t(y) %*% y +
                                     t(m) %*% P %*% m  -
                                     t(m_star) %*% P_star %*% m_star))
    }
    
    ## Sample tau
    tau[i] <- rgamma(1, a_star, b_star)

    ## Sample beta
    beta[i, ] <- MASS::mvrnorm(1, m_star, V_star / tau[i])
  }
  
  ## Conditionally discard warmup-draws
  if (!keep_warmup) {
    tau <- tau[(n_warmup + 1):len_chain]
    beta <- beta[(n_warmup + 1):len_chain, ]
  }
  
  ## Return output
  return(list(beta = beta,
              sigma2 = 1 / tau))
}

## Estimates
# Define 4 seeds
seeds <- sample(10001:99999, 4)

# Run the model with different seeds,collapse to matrix, store as mcmc
gibbs_draws <- lapply(seeds,
                                   function(seed) {
                                     as.mcmc(do.call(
                                       cbind,
                                       lm_gibbs(
                                         m_scalar = 0,
                                         p_scalar = .01,
                                         a = 20,
                                         b = 200,
                                         n_warmup = 5000,
                                         n_draws = 10000,
                                         y = y,
                                         X = X,
                                         seed = seed,
                                         keep_warmup = FALSE
                                       )
                                     ))
                                   })

# Store as mcmc.list
gibbs_draws <- mcmc.list(gibbs_draws)


## ---- export function ----
export <- function(env = environment()) {
  invisible({
    global_obj <- eval(global_objects, envir = globalenv())
    local_obj <- ls(envir = env)
    new_obj <- local_obj[!(local_obj %in% global_obj)]
    sapply(new_obj, function(x) {
      assign(x, get(eval(x, envir = env), envir = env), envir = globalenv())
    })
  })
  if (!is.null(new_obj)){
    cat("Exported objects:\n")
    cat(paste(new_obj, sep = ";; "))
  }
}

global_objects <- c(ls(), "global_objects")
```

## (Joint) probability distributions

### Discrete probability distributions

## Bayesian and frequentist inference: Intepretations

### Credible intervals

You run a Bayesian analysis to estimate an effect of interest. Your goal is to test the hypothesis that the effect of interest is positive.

You calculate the posterior median, the 95% credible interval, and the 99% credible interval.

The values are as follows:

- Posterior median: $1.00$
- 95% credible interval: $[0.02; 1.98]$
- 99% credible interval: $[-0.16; 2.16]$

Frequentists would interpret a result with analogous confidence intervals as "statistically significant at the 5%, but not at the 1% level".

What statements could a Bayesian make about the posterior probability in support of the hypothesis of a positive effect?

### $p$-values

Suppose you have a sample of size $N=150$, from which you recover the sample mean of a continuous variable $X$. Your estimate for the sample mean is $\bar{X} = 129$, your estimate for the standard error of the sample mean is $\hat{\sigma}_{\bar{X}} = 15$.

Let your null hypothesis be that the sample mean is equal to 100. Recover the $p$-value for a two-tailed test. What does this number mean?

```{r}
2 * pnorm(-abs((129 - 100)/ 15))
```

## Bayesian and frequentist inference: Concepts

### Random variables

According to the [English-language Wikipedia](https://en.wikipedia.org/wiki/Random_variable), a *"random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events."*. Its distribution is *"a probability measure on the set of all possible values of the random variable*". We usually think of these distributions in terms of known [probability distributions](https://en.wikipedia.org/wiki/Probability_distribution) (i.e., pmf's or pdf's).

Considering this definition and what you have learned today:

- Why do frequentists think of data, but not of parameters as random variables?
- Why do Bayesians think of parameters, but not of data as random variables?



## Posterior summaries and convergence diagnostics

### Prompt

The following exercises present you with a series of triplets of Markov Chains.

For each exercise,

1. Diagnose the chains for any potential problems using the following tools from the `coda` package:
    1. Trace plots
    1. Autocorrelation plots
    1. Potential scale reduction statistic $\hat{R}$ for within and between chain variance
    1. Heidelberger-Welch diagnostic for within-chain stationarity
1. Interpret your diagnosis: What specific problems of non-convergence, if any, do you find?
1. In the absence of signs of non-convergence, pool the posterior draws across all chains.
    1. Visualize the posterior distribution of the parameter(s)
    1. Provide meaningful posterior summaries (posterior medians, posterior standard deviations, and 95% credible intervals) for the parameter(s)
    
*Hint*: To pool the posterior draws across chains in a `data.frame()`, use `do.call(rbind.data.frame, ...)`, where `...` is a placeholder for the `mcmc.list` object that holds the separate chains.

```{r}
len_chains <- 2000L
num_chains <- 3L

### Chains 1: Well-behaved
chains <- lapply(1:3,
                 rnorm(len_chains, mean = 3, sd = 5)) %>%
  lapply(as.mcmc) %>%
  as.mcmc.list()

### Chains 2: Well-behaved, but highly autocorrelated
chains <- lapply(1:3, function (x) {
  random <- rnorm(len_chains, mean = 3, sd = 5)
  selector <- rep(c(TRUE, rep(FALSE, 9)), 200L)
  random_keep <- c(random[selector], random[len_chains])
  lapply(1:(length(random_keep) - 1L), function(i) {
    tmp <-
      seq(random_keep[i], random_keep[i + 1], length.out = 11L) + rnorm(11L, 0, 0.5)
    return(tmp[1:10])
  }) %>%
    unlist() %>%
    return()
}) %>%
  lapply(as.mcmc) %>%
  as.mcmc.list()

### Chains 3: Between-within variance due to short warm-up
stop_values <- list(
  c(-17, -24, -9, -17, -7, 3),
  c(3, 10, 12, -7, 3, 3),
  c(23, 36, 27, 23, 7, 3)
)
stop_positions <- list(
  c(200, 400, 200, 300, 900),
  c(100, 300, 300, 700, 600),
  c(700, 200, 100, 500, 500)
)
chains <- lapply(1:3, function (x) {
  first <- lapply(1:5, function (y) {
    seq(stop_values[[x]][y], stop_values[[x]][y + 1], length.out = stop_positions[[x]][y]) +
      rnorm(stop_positions[[x]][y], mean = 0, sd = 2)
  }) %>%
    unlist()
  second <- rnorm(len_chains / 2, mean = 3, sd = 2)
  return(c(first, second))
}) %>%
  lapply(as.mcmc) %>%
  as.mcmc.list()

### Chains 4: Stationary, but different
start_vals <- c(-1, 3, 7)
chains <- lapply(1:3, function (x)
                 rnorm(len_chains, mean = start_vals[x], sd = 5)) %>%
  lapply(as.mcmc) %>%
  as.mcmc.list()

### Chains 5: Two parameters, one funky, otherwise well-behaved
chains <- lapply(1:3, function (x)
                 data.frame(
                   par1 = 1 /rgamma(len_chains, 2, 40),
                   par2 = sample(c(
                     rbeta(len_chains, 1, 3),
                     rbeta(len_chains, 3, 1)
                   ),
                   len_chains)
                   )) %>%
  lapply(as.mcmc) %>%
  as.mcmc.list()
```


## Diagnosing, interpreting, and reporting results from a Gibbs sampler for the linear model


### The linear model

Since this is the first regression model we use in a Bayesian estimation framework, it is worthwhile repeating and illustrating the fundamental concepts of Bayesian inference.

### Likelihood

The likelihood gives the *generative model* or *data-generating process* for the outcome, $y$.

The linear model stipulates that the observed outcomes $y_i$ for every unit $i$ can be expressed as realizations from a normal distribution with unit-specific *mean or location parameter* $\mu_i$ and a constant (i.e., general) *variance or scale parameter* $\sigma^2$.

$$y_i \sim \text{N}(\mu_i, \sigma^2) \text{ for all }i = 1,...N$$

or, alternatively,

$$y_i = \mu_i + \epsilon_i  \text{ for all }i = 1,...N \\ \epsilon_i \sim \text{N}(0, \sigma^2)$$
The latter notation makes explicit that each observed $y_i$ can be thought of as a combination of a *systematic component*, $\mu_i$, and a *stochastic error component*, $\epsilon_i$, which follows a zero-mean normal distribution with constant variance $\sigma^2$.

### The systematic component

The systematic component is represented by the mean parameter $\mu_i$. In fact, $\mu_i$ is merely a *transformed parameter*: It is a linear function of unit-specific data $\mathbf{x}_i$ and coefficients $\beta$.

The formula below illustrates this, using the row vector notation $\mathbf{x}_i^{\prime} \beta$ as shorthand for the scalar notation $\beta_1 + \beta_2 x_{i, 2} + ...+\beta_k x_{i,k}$.

$$\mu_i = \underbrace{\mathbf{x}_i^{\prime} \beta}_{= \beta_1 + \beta_2 x_{i, 2} + ...+\beta_k x_{i,k}}  \text{ for all }i = 1,...N$$
### Parameters and priors

Parameters are the unknown quantities in our models whose posterior distributions we seek to infer.

In the linear model, the parameters are all coefficients $\beta$ as well as the variance $\sigma^2$. As a variance parameter, $\sigma^2$ must take on strictly positive values.

In Bayesian data analysis, all parameters must be assigned *priors*. For our Gibbs sampler, we assign uninformative, vague priors for all parameters:

- Independent normal priors for all coefficients: $\beta_k \sim \text{Normal}(0, 100) \text{ for } k = 1,..., K$
- An inverse Gamma prior for the variance: $\sigma^2 \sim \text{Gamma}^{-1}(20, 200)$.

Our Gibbs sampler will then update these distributions and eventually sample from the posterior target distributions from each of the five parameters.

### Data

The exercise chunk has pre-loaded the data frame `gles`, which contains three variables from the 2017 German Longitudinal Election Study. We want to model respondents' support for the AfD (`sup_afd`, measured on an 11-point scale ranging from -5 to 5) as a function of respondents' pro-redistribution preferences (`se_self`) and anti-immigration preferences (`la_self`). 

Both `se_self` and `la_self` are measured on 11-point scales:

- `se_self` ranges from values (0) "less taxes and deductions, even if that means less social spending" to (10) "more social spending, even if that means more taxes and deductions". 
- `la_self` ranges from values (0) "facilitate immigration" to (10) "restrict immigration".

The model formula is given by

$$\mathtt{sup\_afd} = \beta_1 + \beta_2 \mathtt{se\_self} + \beta_3 \mathtt{la\_self} + \epsilon$$
