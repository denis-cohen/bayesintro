---
title: "Lecture: Bayesian Fundamentals"

fontsize: 11pt
linestretch: 1.5

output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: no
    toc: false
    keep_tex: yes

header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{ht}
  - \usepackage[section]{placeins}
  - \usepackage{longtable}
  - \usepackage{hyperref}
  - \hypersetup{colorlinks = true, linkcolor = blue, urlcolor = blue}
  - \widowpenalty10000 
  - \clubpenalty10000
  - \usepackage[page,header]{appendix}
  - \usepackage{titletoc}
  - \usepackage{tocloft}
  - \usepackage{makecell}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

## ---- CRAN Packages ----
## Save package names as a vector of strings
pkgs <-  c("coda", "foreign", "dplyr", "ggplot2", "magick", "ggpubr")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

## ---- GitHub Packages ----


## ---- Global learnr Objects ----
gles <- 
  read.dta("https://github.com/denis-cohen/statmodeling/raw/main/data/gles.dta")

draw_from_prior <-
  function(theta,
           omega,
           alpha,
           beta,
           n_draws,
           seed = 20210329) {
    # Set seed
    set.seed(seed)
    
    # Take draws
    mu <- rnorm(n_draws, theta, 1 / sqrt(omega))
    tau <- rgamma(n_draws, alpha, beta)
    
    ## Return output
    return(list(mu = mu,
                tau = tau))
  }

draw_from_posterior <- function(theta,
                                omega,
                                alpha,
                                beta,
                                n_warmup,
                                n_draws,
                                data,
                                seed = 20210329,
                                keep_warmup = TRUE) {
  # Set seed
  set.seed(seed)

  # Length of chain
  len_chain <- n_warmup + n_draws
  
  # Data characteristics
  n_data <- length(data)  
  mean_data <- mean(data) 

  # Initialize containers
  mu <- rep(NA, len_chain)
  tau <- rep(NA, len_chain)
  
  # Run Gibbs sampler
  for (i in seq_len(len_chain)) {
    if (i == 1) {
      ## Iteration 1: Initialize from prior
      alpha_star <- alpha
      beta_star <- beta
    } else {
      ## Iterations 2+: Update alpha and beta
      alpha_star <- alpha + n_data / 2
      beta_star <- beta + sum(((data - mu[i - 1]) ^ 2) / 2)
    }
    
    ## Sample tau
    tau[i] <- rgamma(1, alpha_star, beta_star)
    
    ## Update theta and omega
    theta_star <-
      (omega * theta + n_data * tau[i] * mean_data) /
      (omega + n_data * tau[i])
    omega_star <- omega + n_data * tau[i]
    
    ## Sample mu
    mu[i] <- rnorm(1, theta_star, 1 / sqrt(omega_star))
  }
  
  ## Conditionally discard warmup-draws
  if (!keep_warmup) {
    tau <- tau[(n_warmup + 1):len_chain]
    mu <- mu[(n_warmup + 1):len_chain]
  }
  
  ## Return output
  return(list(mu = mu,
              tau = tau))
}

seeds <- sample(10001:99999, 4)
draws_multiple_chains <- lapply(seeds,
                                function(seed) {
                                  as.mcmc(simplify2array(
                                    draw_from_posterior(
                                      theta = 0,
                                      omega = .1,
                                      alpha = 20,
                                      beta = 200,
                                      n_warmup = 200,
                                      n_draws = 6147,
                                      data = gles$sup_afd,
                                      keep_warmup = FALSE,
                                      seed = seed
                                    )
                                  ))
                                })

# Save as mcmc.list
draws_multiple_chains <- as.mcmc.list(draws_multiple_chains)
```

## Bayesian fundamentals

### Parameters as random variables

<blockquote>

<sub> In the Bayesian world the unobserved quantities are assigned
distributional properties and, therefore, become random variables in the
analysis. </sub> <br>

<sub> These distributions come in two basic flavors. If the distribution
of the unknown quantity is not conditioned on fixed data, it is called
prior distribution because it describes knowledge prior to seeing data.
</sub> <br>

<sub> Alternatively, if the distribution is conditioned on data that we
observe, it is clearly updated from the unconditioned state and,
therefore, more informed. This distribution is called posterior
distribution.</sub>

</blockquote>

::: {style="text-align: right"}
<sub><sup> [Gill, J., & Witko, C. (2013). Bayesian analytical methods: A
methodological prescription for public administration. Journal of Public
Administration Research and Theory, 23(2),
457--494.](https://academic.oup.com/jpart/article/23/2/457/1003493)
</sub></sup>
:::

### Contrast to the frequentist paradigm

Thus, the data is *fixed* and the parameter is a *random variable* that
can be characterized in terms of distributional properties.

Contrast this with the frequentist the approach:

-   The unknown *true* population parameter is fixed.
-   The data is random: Each sample is a stochastic draw from a
    population.
-   Therefore, any estimator/statistic is a random variable, too.
-   The sampling distribution characterizes the probability distribution
    of such estimators and statistics, usually in terms of a normal
    distribution. - Since the true mean and variance of the sampling
    distribution are unknown, we substitute them with the analogous
    sample estimates.

### Bayesian updating

Bayesian updating follows the following idea:

<blockquote>

$$
\text{prior beliefs} \rightarrow \text{data} \rightarrow \text{posterior beliefs}
$$

<sub> or equivalently, </sub>

$$
p(\theta) \rightarrow y \rightarrow p(\theta| y)
$$

</blockquote>

::: {style="text-align: right"}
<sub><sup> [Jackman, S. (2009). Bayesian Analysis for the Social
Sciences.
Wiley.](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470686621)
</sub></sup>
:::

Prior beliefs can be vague -- if we know (hardly) anything about a
parameter $\theta$ prior to analyzing the data $y$ -- or specific,
indicating we hold strong prior beliefs about $\theta$ even before
seeing the data.

It is important to note that we express our beliefs about everything --
prior, data, and posterior -- *distributionally*:

-   The prior and posterior distributions give probability distribution
    for $\theta$ before/after updating our knowledge by analyzing the
    data.
-   A *likelihood function*, $p(y|\theta)$, gives a generative model
    that stipulates how the data $y$ are linked to a parameter $\theta$.

Let's think about this last point: How does it play out in the case of a
single coin flip?

### Bayes' theorem

[Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) lies at
the core of Bayesian inference. It tells us how to update our prior
beliefs upon seeing the data to arrive at posterior beliefs.

Bayes' theorem states that

$$
p(\text{hypothesis|evidence}) = \frac{p(\text{hypothesis}) \times p(\text{evidence|hypothesis}) }{p(\text{evidence})}
$$

### Bayes theorem: Proportional version

Since the data (or "evidence") is considered fixed and not a random
variable, Bayes' theorem simplifies to its proportional form:

$$
\underbrace{p(\text{hypothesis|evidence})}_{\text{posterior belief}} \propto \underbrace{p(\text{hypothesis})}_{\text{prior belief}} \times \underbrace{p(\text{evidence|hypothesis})}_{\text{likelihood function}}
$$

or

$$
p(\theta | \mathbf{y}) \propto p(\theta) \times p(\mathbf{y}|\theta)
$$

### Likelihood function

-   Specification of a pdf or pmf for the *outcome*, $y$:
    $p(\mathbf{y}|\theta)$.
-   Also called the data generating process (or the generative model)
    for $y$.
-   Note that the likelihood function multiplies densities across *all*
    observations; e.g., a normal likelihood function is given by:

$$p(\mathbf{y}|\mu, \sigma) = \prod_{i=1}^{N} \text{Normal}\left(y_i | \mu_i, \sigma \right)$$

-   This is what we mean mathematically when we use the shorthand
    $\mathbf{y} \sim \text{Normal}(\mu, \sigma)$

### Prior distribution

-   A distributional characterization of our belief about an unknown
    quantity (i.e., a *parameter*) prior to seeing the data: $p(\theta)$
-   This includes statements about *family*, *support*, and *density*.
    -   *Family*: A pdf (continuous parameters) or pmf (discrete
        parameters) that can plausibly generate the parameter values.
    -   *Support*: Some parameters have constrained support: Probability
        parameters must be inside $[0, 1]$; variance parameters must be
        $\geq 0$.
    -   *Density*: A distributional characterization which values of the
        parameter we think are more or less likely to observe.
-   The prior distribution can be
    -   flat (i.e., uniformly distributed over the supported range)
    -   purposefully vague, and thus, rather uninformative
    -   weakly informative
    -   specific and substantively informed (e.g., by previous research
        or expert assessment)

### Posterior distribution

-   Updating our distributional belief about $\theta$ given the data,
    $\mathbf{y}$: $p(\theta | \mathbf{y})$
-   Follows the proportional version of [Bayes'
    theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):
    $p(\theta | \mathbf{y}) \propto p(\theta) \times p(\mathbf{y}|\theta)$
-   Yields a weighthed combination of likelihood and prior
-   The prior pulls the posterior density toward the center of gravity
    of the prior distribution
-   As the data grows large, the likelihood becomes more influential:
    -   one factor for $p(\theta)$, $N$ factors for $p(y_i|\theta_i)$
    -   we will see this analytically and using simulations later on

## Coin flip experiment

### The experiment

Suppose we flip a coin up to $N$ times:

-   The fairness of a coin can be expressed through a *probability
    parameter*, $\pi$, that governs the probability that a coin flip
    produces heads (1) has opposed to tails (0)
-   We start out with the belief that the coin is fair -- that is, we
    consider it more probable that the coin is fair ($\pi \approx 0.5$)
    and less probable that it systematically over-produces either heads
    or tails
-   Unbeknownst to us, the coin is far from fair -- it is 4 times as
    likely to produce heads as it is to produce tails (that is,
    $\pi=0.8$)
-   We slowly learn about this in the process of flipping the coin and
    keeping score of the number of flips $n$ and the number of heads
    $k$...

### Analytical form: Prior distribution

-   The *beta distribution* is a suitable candidate for characterizing
    our prior beliefs: $\pi \sim \text{beta}(a,b)$
-   Characterized by two shape parameters, $a$ and $b$
-   $a$ and $b$ are *hyperparameters*: Known (or chosen) parameters that
    characterize a prior distribution.
-   Constrained support: $\pi \in [0, 1]$
-   pdf: $p(\pi) = \frac{\pi^{a-1} (1- \pi)^{b-1}}{\text{B}(a, b)}$

```{r beta, fig.align='center', fig.width = 6, fig.height = 6}
len_pi <- 1001L                      ### number of candidate values for pi

## Plot
par(mfrow = c(2, 2))

pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 0.95                       ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = 'n',
  main = "Beta(0.95, 0.95)  \n (weak, symmetrical, bimodal)",
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi)),
  ylim = c(0, 4),
  axes = F
)
axis(1)
axis(2)
polygon(                             ### draw density distribution
  c(0, pi[!is.infinite(prior)], 1),
  c(0, prior[!is.infinite(prior)], 0),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = 'white'
)

pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 1.0                        ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = 'n',
  main = "Beta(1.0, 1.0)  \n (flat, uniform)",
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi)),
  ylim = c(0, 4),
  axes = F
)
axis(1)
axis(2)
polygon(                             ### draw density distribution
  c(0, pi[!is.infinite(prior)], 1),
  c(0, prior[!is.infinite(prior)], 0),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = 'white'
)

pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 1.1                        ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = 'n',
  main = "Beta(1.1, 1.1) \n (weak, symmetrical, unimodal)",
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi)),
  ylim = c(0, 4),
  axes = F
)
axis(1)
axis(2)
polygon(                             ### draw density distribution
  c(rep(0, length(pi)), pi),
  c(rev(prior), prior),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = 'white'
)

pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 10                        ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = 'n',
  main = "Beta(10, 10)  \n (stronger, symmetrical, unimodal)",
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi)),
  ylim = c(0, 4),
  axes = F
)
axis(1)
axis(2)
polygon(                             ### draw density distribution
  c(rep(0, length(pi)), pi),
  c(rev(prior), prior),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = 'white'
)
```

### Analytical form: Likelihood

-   Flipping one and the same coin $n$ times is a series of Bernoulli
    trials
-   The *binomial distribution* describes the corresponding data
    generating process: $k \sim \text{Binomial}(n, \pi)$
-   pmf: $p(k|n, \pi) = {n \choose k} \pi^k (1-\pi)^{(n-k)}$

### Analytical form: Posterior distribution

Remember:
$$p(\theta | \mathbf{y}) \propto p(\theta) \times p(\mathbf{y}|\theta)$$

So what does this mean in the present example?

$$\begin{split}p(\pi|n,k) & \propto p(\pi) \times p(k|n, \pi) \\
 p(\pi|n,k) & \propto \frac{\pi^{a-1} (1- \pi)^{b-1}}{\text{B}(a, b)} \times {n \choose k} \pi^k (1-\pi)^{(n-k)}\end{split}$$

Note that since we use the proportional version of Bayes' Law (i.e., we
do not stipulate exact equality), we can drop any constant terms that do
not involve our parameter of interest, $\pi$:

$$\begin{split}p(\pi|n,k) & \propto \pi^{a-1} (1- \pi)^{b-1} \times \pi^k (1-\pi)^{(n-k)}\end{split}$$
The rest, then, is easy: Following the rules of exponentiation, we add
exponents for identical bases. This gives us our posterior distribution
for $\pi$:

$$\begin{split}p(\pi|n,k) & \propto \pi^{a+k-1} (1- \pi)^{b+n-k-1}\end{split}$$
As you see, our posterior has the exact same form as our prior. It is a
beta distribution with updated parameters

-   $a^{\prime} = a+k-1$
-   $b^{\prime} = b+n-k-1$

This property is called *conjugacy*: Prior and posterior are of the same
family.

Now, take a moment to think about our analytical solution for the
updated parameter:

-   What does it take for the data to dominate the prior?
-   What if the prior is weak (e.g.,
    $\pi \sim \text{beta}(a = 1,b = 1)$)?
-   What if the prior is strong (e.g.,
    $\pi \sim \text{beta}(a = 100, b = 100)$)?

### Simulation

#### Prior distribution

<details>

<summary>Code: Defining and plotting the prior distribution</summary>

```{r coin-sim0, eval = FALSE, echo = TRUE}
len_pi <- 1001L                      ### number of candidate values for pi
pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 5                          ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution

## Plot
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = 'n',
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi))
)
polygon(                             ### draw density distribution
  c(rep(0, length(pi)), pi),
  c(prior, rev(prior)),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = 'white'
)
```

</details>

```{r coin-sim0-print, eval = TRUE, echo = FALSE, fig.align='center', out.width='75%'}
len_pi <- 1001L                      ### number of candidate values for pi
pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 5                          ### hyperparameters
prior <- dbeta(pi, a, b)             ### prior distribution

## Plot
plot(
  pi, prior,
  type = 'n',
  xlab = "Density",
  ylab = expression(paste("Prior Distribution for ", pi))
)
polygon(
  c(rep(0, length(pi)), pi),
  c(prior, rev(prior)),
  col = adjustcolor('red', alpha.f = .4),
  border = NA
)
abline(
  v = .5,
  col = 'white'
)
```

#### Posterior distribution

<details>

<summary>Code: Simulating the experiment</summary>

```{r coin-sim1, eval = FALSE, echo = TRUE}
set.seed(20210329)                   ### set seed for replicability
len_pi <- 1001L                      ### number of candidate values for pi
pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 5                          ### hyperparameters
n <- 300                             ### num. of coin flips
pi_true <- .8                        ### true parameter
data <- rbinom(n, 1, pi_true)        ### n coin flips
posterior <- matrix(NA, 3L, n)       ### matrix container for posterior

for (i in seq_len(n)) {    
  current_sequence <- data[1:i]      ### sequence up until ith draw
  k <- sum(current_sequence)         ### number of heads in current sequence
  
  ##### Updating
  a_prime <- a + k               
  b_prime <- b + i - k
  
  ### Analytical means and credible intervals
  posterior[1, i] <- a_prime / (a_prime + b_prime)
  posterior[2, i] <- qbeta(0.025, a_prime, b_prime)
  posterior[3, i] <- qbeta(0.975, a_prime, b_prime)
}

## Plot
plot(                                ### set up empty plot with labels
  1:n, 1:n,
  type = 'n',
  xlab = "Number of Coin Flips",
  ylab = expression(paste("Posterior Means of ",
                          pi,
                          sep = " ")), 
  ylim = c(0, 1),
  xlim = c(1, n)
)
abline(                              ### reference line for the true pi
  h = c(.5, .8),
  col = "gray80"
)
rect(-.5, qbeta(0.025, 5, 5),        ### prior mean + interval at i = 0
     0.5, qbeta(0.975, 5, 5),
     col = adjustcolor('red', .4),
     border = adjustcolor('red', .2))
segments(-.5, .5,
         0.5, .5,
         col = adjustcolor('red', .9),
         lwd = 1.5)
polygon(                             ### posterior means + intervals
  c(seq_len(n), rev(seq_len(n))),
  c(posterior[2, ], rev(posterior[3, ])),
  col = adjustcolor('blue', .4),
  border = adjustcolor('blue', .2)
)
lines(
  seq_len(n),
  posterior[1, ],
  col = adjustcolor('blue', .9),
  lwd = 1.5
)
```

</details>

```{r coin-sim2, eval = TRUE, echo = FALSE, fig.align='center', out.width='75%'}
set.seed(20210329)                   ### set seed for replicability
len_pi <- 1001L                      ### number of candidate values for pi
pi <- seq(0, 1, length.out = len_pi) ### candidate values for pi
a <- b <- 5                          ### hyperparameters
n <- 300                             ### num. of coin flips
pi_true <- .8                        ### true parameter
data <- rbinom(n, 1, pi_true)        ### n coin flips
posterior <- matrix(NA, 3L, n)       ### matrix container for posterior

for (i in seq_len(n)) {    
  current_sequence <- data[1:i]      ### sequence up until ith draw
  k <- sum(current_sequence)         ### number of heads in current sequence
  
  ##### Updating
  a_prime <- a + k               
  b_prime <- b + i - k
  
  ### Analytical means and credible intervals
  posterior[1, i] <- a_prime / (a_prime + b_prime)
  posterior[2, i] <- qbeta(0.025, a_prime, b_prime)
  posterior[3, i] <- qbeta(0.975, a_prime, b_prime)
}

## Plot
plot(
  1:n, 1:n,
  type = 'n',
  xlab = "Number of Coin Flips",
  ylab = expression(
    paste("Posterior Means of ",
          pi,
          sep = " ")
  ), 
  ylim = c(0, 1),
  xlim = c(1, n)
)
abline(
  h = c(.5, .8),
  col = "gray80"
)
rect(-.5, qbeta(0.025, 5, 5),
     0.5, qbeta(0.975, 5, 5),
     col = adjustcolor('red', .4),
     border = adjustcolor('red', .2))
segments(-.5, .5,
         0.5, .5,
         col = adjustcolor('red', .9),
         lwd = 1.5)
polygon(
  c(seq_len(n), rev(seq_len(n))),
  c(posterior[2, ], rev(posterior[3, ])),
  col = adjustcolor('blue', .4),
  border = adjustcolor('blue', .2)
)
lines(
  seq_len(n),
  posterior[1, ],
  col = adjustcolor('blue', .9),
  lwd = 1.5
)
```

*Note:* After `r n` coin flips, we have observed `r k` heads, which is a
proportion of `r round(k / n, 3)`. The posterior median is
`r round(posterior[1, ncol(posterior)], 3)`; the 95% credible interval
is [`r round(posterior[2, ncol(posterior)], 3)`,
`r round(posterior[3, ncol(posterior)], 3)`].

## MCMC algorithms

### Analytical (classical) Bayesian inference

-   As you may have noticed: Our coin flip example did *not* involve
    *any* numerical estimation algorithms.
-   We simply observed the data, applied Bayes' Law, and analytically
    updated our parameters.
-   This allowed us to retrieve a distributional characterization of our
    parameter of interest at each iteration of the coin flip series.
-   The reasons why we could do this with ease is that this simple
    Binomial problem involved a single parameter $\pi$; i.e, we were
    dealing with a uni-dimensional *parameter space*.

### The limits of analytical Bayesian inference

-   Even in only slightly more intricate applications, Bayesian
    inference involves finding a *joint* posterior for *all* parameters
    in a model, i.e., finding a *multi-dimensional* parameter space.
-   Inference on single parameters from a joint multi-dimensional
    parameter space requires that we retrieve the marginal posterior
    distribution from the joint posterior distribution.
-   Marginalizing the joint multidimensional posterior distribution
    w.r.t. to a given a parameter gives the posterior distribution for
    that parameter. This requires *integrating* out all other
    parameters.
-   For instance, when our joint posterior in a three-dimensional
    parameter space is $p(\alpha,\beta, \gamma)$, we need to obtain each
    marginal posterior akin to
    $p(\alpha) = \int_{\beta} \int_{\gamma} p(\alpha,\beta, \gamma) d\beta d\gamma$
-   For complex multi-dimensional posterior distributions, finding
    analytical solutions through integration becomes cumbersome, if not
    outright impossible.

### Numerical approximation via MCMC

That's where numerical approximation through Markov Chain Monte Carlo
(MCMC) algorithms comes in:

-   MCMC are iterative computational processes that explore and describe
    a posterior distribution.
-   Developed in the 1980s and popularized in the 1990s, MCMC algorithms
    quickly eliminated the need for analytical marginalizations of
    single parameters from joint multi-dimensional posteriors.
-   The core idea:
    -   *Markov Chains* wander through, and take samples from, the
        parameter space.Following an initial warmup period, the Markov
        Chains will converge to high-density regions of the underlying
        posterior distribution (ergodicity).
    -   The proportion of "steps" in a given region of multidimensional
        parameter space gives a stochastic simulation of the posterior
        probability density.
    -   This yields a numerical approximation of the underlying
        posterior distribution, much like Monte Carlo simulations of MLE
        parameters yield numerical approximations of the underlying
        sampling distribution.

### (Some) MCMC Algorithms

1.  **Gibbs**: Draws iteratively and alternatively from the conditional
    conjugate distribution of each parameter.
2.  **Metropolis-Hastings**: Considers a single multidimensional move on
    each iteration depending on the quality of the proposed candidate
    draw.
3.  **Hamiltonian Monte Carlo (HMC)**, used in Stan:

<blockquote><sub> The Hamiltonian Monte Carlo algorithm starts at a
specified initial set of parameters $\theta$; in Stan, this value is
either user-specified or generated randomly. Then, for a given number of
iterations, a new momentum vector is sampled and the current value of
the parameter $\theta$ is updated using the leapfrog integrator with
discretization time $\epsilon$ and number of steps $L$ according to the
Hamiltonian dynamics. Then a Metropolis acceptance step is applied, and
a decision is made whether to update to the new state
$(\theta^{\ast},\rho{\ast})$ or keep the existing state.
</sub></blockquote>

::: {style="text-align: right"}
<sub><sup> Source: [Stan Reference Manual, Section
14.1](https://mc-stan.org/docs/2_19/reference-manual/hamiltonian-monte-carlo.html)
</sub></sup>
:::

### Resource: Animated visualizations of several MCMC algorithms

[Chi Feng](https://github.com/chi-feng) hosts animated visualizations of several MCMC algorithms on his [website](https://chi-feng.github.io/mcmc-demo/app.html). Worth a visit!

## Gibbs sampler

### In a nutshell

-   MCMC algorithms can be complex.

-   Here, we will illustrate the intuition of sampling from the joint
    posterior of all parameters using the arguably most straightforward
    MCMC algorithm: The *Gibbs sampler*.

-   Remember that Gibbs draws iteratively and alternatively from the
    conditional conjugate distribution of each parameter.

-   We want to perform inference on a variable $y$, of which we have $N$
    observations.

-   We stipulate that the data-generating process that produces $y$ is
    normal: $\mathbf{y} \sim \text{N}(\mu, \sigma^2)$. This yields a
    *two-dimensional parameter space* -- i.e., a bivariate posterior
    distribution.

### Application

We will focus on the variable `sup_afd` from the data set `gles`.

Let's pretend our prior belief is very uninformed:

-   We don't know how (un)popular the AfD is in the German electorate
-   But we know that individual support is measured on a -5 to 5 scale
-   Our prior belief for $\mu$ should thus be agnostic as to whether
    people like or dislike the AfD and sufficiently vague to allow for
    the possibility that we may be wrong:
    $\mu \sim \text{Normal}(\theta = 0, \omega^{2} = 4)$ (mean $\theta$
    and variance $\omega ^ 2$ are hyperparameters for the normal prior
    distribution of $\mu$)
-   Our prior belief for $\tau$ will also be vague:
    $\sigma^2 \sim \text{Gamma}^{-1}(\alpha = 2, \beta = 10)$ (shape
    $\alpha$ and rate $\beta$ are hyperparameters for the inverse Gamma
    prior distribution of $\tau$)
-   We have no prior belief about the dependence of both parameters and
    hence specify independent prior distributions

### Analytical prerequisites

-   A Gibbs sampler requires analytical solutions -- mathematical
    formulas -- for the *conditional* posterior distributions of the two
    parameters from whose marginal posteriors we would like to sample.
-   Note that this does *not* involve marginalizing out the "unwanted"
    parameters; instead, we will derive the posteriors of $\mu$ and
    $\sigma^2$ as conditional functions of each other, respectively.
-   Put simply, the updated posterior of $\mu$ depends on values of
    $\sigma ^ 2$, and the updated posterior of $\sigma ^ 2$ depends on
    values of $\mu$.

### Sampling

-   We first initialize $\sigma_1^2$ by taking a random draw from its
    prior distribution.
-   We then initialize $\mu_1$ by taking a draw from its updated
    posterior distribution, conditional on $\sigma_1^2$
-   We then sample iteratively and alternatively:
    -   Draw $\sigma_2^2$ from its updated posterior distribution,
        conditional on $\mu_1$
    -   Draw $\mu_2$ from its updated posterior distribution,
        conditional on $\sigma_2^2$
    -   Draw $\sigma_s^2$ from its updated posterior distribution,
        conditional on $\mu_{s-1}$ for $s \in \{3,...,S\}$
    -   Draw $\mu_s$ from its updated posterior distribution,
        conditional on $\sigma_s^2$ for $s \in \{3,...,S\}$
-   Eventually, after a sufficiently large number of simulations $S$,
    the sampler converges to its *posterior target distribution*.
-   Once converged, any draws sampled from the posterior target
    distributions give accurate *numerical simulations* of the joint
    posterior distribution of $\mu$ and $\sigma^2$.

### Step 1: Understanding the prior distributions of $\mu$ and $\sigma^2$

```{r gibbs-functions, echo = FALSE}
# Auxiliaries
len_warmup = 50L
len_sample = 1000L

# Draw from prior
draw_from_prior <-
  function(theta,
           omega,
           alpha,
           beta,
           n_draws,
           seed = 20210329) {
    # Set seed
    set.seed(seed)
    
    # Take draws
    mu <- rnorm(n_draws, theta, 1 / sqrt(omega))
    tau <- rgamma(n_draws, alpha, beta)
    
    # Return output
    return(list(mu = mu,
                tau = tau))
  }

# Draw from posterior
# Define function
draw_from_posterior <- function(theta,
                                omega,
                                alpha,
                                beta,
                                n_warmup,
                                n_draws,
                                data,
                                seed = 20210329,
                                keep_warmup = TRUE) {
  # Set seed
  set.seed(seed)

  # Length of chain
  len_chain <- n_warmup + n_draws
  
  # Data characteristics
  n_data <- length(data)  
  mean_data <- mean(data) 

  # Initialize containers
  mu <- rep(NA, len_chain)
  tau <- rep(NA, len_chain)
  
  # Run Gibbs sampler
  for (i in seq_len(len_chain)) {
    if (i == 1) {
      ## Iteration 1: Initialize from prior
      alpha_star <- alpha
      beta_star <- beta
    } else {
      ## Iterations 2+: Update alpha and beta
      alpha_star <- alpha + n_data / 2
      beta_star <- beta + sum(((data - mu[i - 1]) ^ 2) / 2)
    }
    
    ## Sample tau
    tau[i] <- rgamma(1, alpha_star, beta_star)
    
    ## Update theta and omega
    theta_star <-
      (omega * theta + n_data * tau[i] * mean_data) /
      (omega + n_data * tau[i])
    omega_star <- omega + n_data * tau[i]
    
    ## Sample mu
    mu[i] <- rnorm(1, theta_star, 1 / sqrt(omega_star))
  }
  
  ## Conditionally discard warmup-draws
  if (!keep_warmup) {
    tau <- tau[(n_warmup + 1):len_chain]
    mu <- mu[(n_warmup + 1):len_chain]
  }
  
  ## Return output
  return(list(mu = mu,
              tau = tau))
}

# Apply function for prior
draws_prior <-
  draw_from_prior(
    theta = 0,
    omega = .25,
    alpha = 2,
    beta = 10,
    n_draws = 10000
  )
prior_data <- as_tibble(draws_prior) %>%
  mutate(sigma2 = 1 / tau,
         i = row_number())

# Apply function for posterior
draws_posterior <-
  draw_from_posterior(
    theta = 0,
    omega = .25,
    alpha = 2,
    beta = 10,
    n_warmup = len_warmup,
    n_draws = len_sample,
    data = gles$sup_afd,
    keep_warmup = TRUE
  )
posterior_data <- as_tibble(draws_posterior) %>%
  mutate(sigma2 = 1 / tau,
         i = row_number())
```

```{r prior-plot, fig.align='center', fig.width = 6, fig.height = 6}
## ---- Step 0: Auxiliaries ----
mu_limits <- c(-5, 5)
sigma2_limits <- c(0, 25)

mu_zoom_limits <- c(-3.5, -3)
sigma2_zoom_limits <- c(4, 9)

## ---- Step 1: The prior ----
## Marginal mu
prior_mu <- ggplot(data = data.frame(x = c(-100, 100)), aes(x)) +
  stat_function(fun = dnorm,
                n = 10001,
                args = list(mean = 0, sd = 2),
                geom = "polygon",
                color = "black",
                fill = "red",
                alpha = 0.25) + 
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL)  +
  xlim(mu_limits) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

## Marginal sigma2
dinvgamma <- function(x, shape, rate) {
  rate ^ shape / gamma(shape) * x ^ (-1 - shape) * exp(-rate / x)
}

prior_sigma2 <- ggplot(data = data.frame(x = c(0, 100)), aes(x)) +
  stat_function(
    fun = function(x, shape, rate)
      dinvgamma(x = x, shape = shape, rate = rate),
    n = 10001,
    args = list(shape = 2, rate = 10),
    geom = "polygon",
    color = "black",
    fill = "red",
    alpha = 0.25
  ) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  xlim(sigma2_limits) +
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

## Joint density
prior_density <-
  prior_data %>%
  ggplot(aes(x = mu, y = sigma2)) +
  stat_density_2d(aes(fill = ..density..), geom = "raster", contour = FALSE) +
  scale_fill_viridis_c() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlab(expression(mu)) +
  ylab(expression(sigma ^ 2))   +
  theme(legend.position = "none") +
  xlim(mu_limits) +
  ylim(sigma2_limits) +
  theme(plot.margin = unit(c(0, 0, 1, 1), 'lines'))

## Multiplot
ggarrange(
  prior_mu,
  NULL,
  prior_density,
  prior_sigma2,
  nrow = 2,
  ncol = 2,
  align = "hv",
  widths = c(2, 1),
  heights = c(1, 2)
) %>%
  annotate_figure(top = text_grob("Prior distributions"))
```

### Step 2a: Initializing $\sigma^2$

We draw $\sigma_1^2$ from its prior distribution:

```{r initialize-tau, fig.align='center', fig.width = 6, fig.height = 6}
## ---- Initialization ----
## Trace canvas
prior_trace_mu <- ggplot(data = posterior_data %>%
                     dplyr::slice(1),
                   aes(x = i,
                       y = mu)) +
  geom_point() +
  geom_rect(aes(
    xmin = 1, 
    xmax = len_warmup,
    ymin = mu_limits[1],
    ymax = mu_limits[2]
  ),
  fill = "gray80",
  alpha = 0.1,
  color = NA) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup) +
  ylim(mu_limits) + 
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

prior_trace_sigma2 <- ggplot(data = posterior_data %>%
                     dplyr::slice(1),
                   aes(x = i,
                       y = sigma2)) +
  geom_point() +
  geom_rect(aes(
    xmin = 1, 
    xmax = len_warmup,
    ymin = sigma2_limits[1],
    ymax = sigma2_limits[2]
  ),
  fill = "gray80",
  alpha = 0.1,
  color = NA) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup) +
  ylim(sigma2_limits) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

## Multiplot
ggpubr::ggarrange(
  prior_mu,
  NULL,
  prior_density +
    geom_hline(yintercept = posterior_data$sigma2[1],
               col = "white"),
  prior_sigma2,
  nrow = 2,
  ncol = 2,
  align = "hv",
  widths = c(2, 1),
  heights = c(1, 2)
) %>%
  annotate_figure(top = text_grob(expression("Initialize" ~ sigma^2)))
```

### Step 2b: Initializing $\mu$

We draw $\mu_1$ from the conditional conjugate posterior, given
$\sigma_1^2$:

```{r initialize-mu, fig.align='center', fig.width = 6, fig.height = 6}
ggpubr::ggarrange(
  prior_mu,
  NULL,
  prior_density +
    geom_hline(yintercept = posterior_data$sigma2[1],
               col = "white") +
    geom_vline(xintercept = posterior_data$mu[1],
               col = "white") +
    geom_point(x = posterior_data$mu[1],
               y = posterior_data$sigma2[1],
               col = "white",
               alpha = .25),
  prior_sigma2,
  nrow = 2,
  ncol = 2,
  align = "hv",
  widths = c(2, 1),
  heights = c(1, 2)
) %>%
  annotate_figure(top = text_grob(expression("Initialize" ~ sigma^2 ~ "and" ~ mu)))
```

### Step 3: Sampling warm-up draws

We run the Gibbs sampler for an initial period of fifty draws.

```{r warmup, fig.align='center', fig.width = 6, fig.height = 6}
warmup_density <- posterior_data %>%
  dplyr::mutate_at(
    .vars = vars(mu, sigma2),
    .funs = list(lag = ~ ifelse(i == 1, ., dplyr::lag(.)))
  ) %>%
  dplyr::slice(1:len_warmup) %>%
  ggplot(aes(x = mu, y = sigma2)) +
  stat_density_2d(aes(fill = ..density..), geom = "raster", contour = FALSE) +
  scale_fill_viridis_c() +
  geom_point(aes(x = mu,
                 y = sigma2),
             color = "white",
             alpha = 0.33,
             size = 0.75) +
  geom_segment(
    aes(
      x = mu,
      xend = mu_lag,
      y = sigma2,
      yend = sigma2
    ),
    color = "white",
    alpha = 0.33,
    lwd = 0.5
  ) +
  geom_segment(
    aes(
      x = mu_lag,
      xend = mu_lag,
      y = sigma2,
      yend = sigma2_lag
    ),
    color = "white",
    alpha = 0.33,
    lwd = 0.5
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlab(expression(mu)) +
  ylab(expression(sigma ^ 2))   +
  theme(legend.position = "none") +
  xlim(mu_limits) +
  ylim(sigma2_limits)

warmup_trace_mu <- ggplot(data = posterior_data %>%
                     dplyr::slice(1:len_warmup),
                   aes(x = i,
                       y = mu)) +
  geom_rect(
    aes(
      xmin = 1,
      xmax = len_warmup,
      ymin = mu_limits[1],
      ymax = mu_limits[2]
    ),
    fill = "gray80",
    alpha = 0.1,
    color = NA
  ) +
  geom_line(lwd = .3) +
  # geom_smooth(col = "white",
  #             se = FALSE) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup) +
  ylim(mu_limits) + 
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

warmup_trace_sigma2 <- ggplot(data = posterior_data %>%
                     dplyr::slice(1:len_warmup),
                   aes(x = i,
                       y = sigma2)) +
  geom_rect(aes(
    xmin = 1, 
    xmax = len_warmup,
    ymin = sigma2_limits[1],
    ymax = sigma2_limits[2]
  ),
  fill = "gray80",
  alpha = 0.1,
  color = NA) +
  geom_line(lwd = .3) +
  # geom_smooth(col = "white",
  #             se = FALSE) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup) +
  ylim(sigma2_limits) +
  theme(
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

warmup_histogram_sigma2 <- posterior_data %>%
  dplyr::slice(1:len_warmup) %>%
  ggplot(aes(x = sigma2)) +
  geom_histogram(bins = 50) +
  xlim(sigma2_limits) +
  ylim(0, 50) + 
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

warmup_histogram_mu <- posterior_data %>%
  dplyr::slice(1:len_warmup) %>%
  ggplot(aes(x = mu)) +
  geom_histogram(bins = 50) +
  xlim(mu_limits) +
  ylim(0, 50) + 
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))


ggpubr::ggarrange(
  warmup_histogram_mu,
  NULL,
  NULL,
  warmup_trace_mu,
  NULL,
  NULL,
  warmup_density,
  warmup_trace_sigma2,
  warmup_histogram_sigma2,
  nrow = 3,
  ncol = 3,
  align = "hv",
  widths = c(2, 1, 1),
  heights = c(1, 1, 2)
) %>%
  annotate_figure(top = text_grob(expression("Warm-up sampling: 50 draws")))
```

### Posterior regions of interest

Upon closer inspection, we see that the Gibbs sampler immediately
converges to the posterior target distribution.

Unlike our very vague prior, the posterior conveys specific knowledge:
All draws are within the yellow lines.

```{r warmup-find, fig.align='center', fig.width = 6, fig.height = 6}
ggpubr::ggarrange(
  warmup_histogram_mu +
    geom_vline(xintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = mu_zoom_limits[2],
               col = "yellow"),
  NULL,
  NULL,
  warmup_trace_mu +
    geom_hline(yintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = mu_zoom_limits[2],
               col = "yellow"),
  NULL,
  NULL,
  warmup_density +
    geom_vline(xintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = mu_zoom_limits[2],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  warmup_trace_sigma2 +
    geom_hline(yintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  warmup_histogram_sigma2 +
    geom_vline(xintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  nrow = 3,
  ncol = 3,
  align = "hv",
  widths = c(2, 1, 1),
  heights = c(1, 1, 2)
) %>%
  annotate_figure(top = text_grob("Warm-up sampling: 50 draws (see where the action happens)"))
```

### Zooming in

We therefore "zoom in" on the rectangular area inbetween the yellow
lines.

```{r warmup-find-zoom, fig.align='center', fig.width = 6, fig.height = 6}
ggpubr::ggarrange(
  warmup_histogram_mu +
    xlim(mu_zoom_limits) +
    geom_vline(xintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = mu_zoom_limits[2],
               col = "yellow"),
  NULL,
  NULL,
  warmup_trace_mu  +
    ylim(mu_zoom_limits) +
    geom_hline(yintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = mu_zoom_limits[2],
               col = "yellow"),
  NULL,
  NULL,
  warmup_density +
    xlim(mu_zoom_limits) +
    ylim(sigma2_zoom_limits) +
    geom_vline(xintercept = mu_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = mu_zoom_limits[2],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  warmup_trace_sigma2 +
    ylim(sigma2_zoom_limits) +
    geom_hline(yintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_hline(yintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  warmup_histogram_sigma2 +
    xlim(sigma2_zoom_limits) +
    geom_vline(xintercept = sigma2_zoom_limits[1],
               col = "yellow") +
    geom_vline(xintercept = sigma2_zoom_limits[2],
               col = "yellow"),
  nrow = 3,
  ncol = 3,
  align = "hv",
  widths = c(2, 1, 1),
  heights = c(1, 1, 2)
) %>%
  annotate_figure(top = text_grob("Warm-up sampling: 50 draws (see where the action happens)"))
```

### Step 4: Run the sampler for at least 1000 post-warmup iterations

```{r posterior-full, fig.align='center', fig.width = 6, fig.height = 6}
posterior_density <- posterior_data %>%
      ggplot(aes(x = mu, y = sigma2)) +
      stat_density_2d(aes(fill = ..density..),
                      geom = "raster",
                      contour = FALSE) +
      scale_fill_viridis_c() +
      scale_x_continuous(expand = c(0, 0)) +
      scale_y_continuous(expand = c(0, 0)) +
      xlab(expression(mu)) +
      ylab(expression(sigma ^ 2))   +
      theme(legend.position = "none") +
      xlim(mu_zoom_limits) +
      ylim(sigma2_zoom_limits)
  
  posterior_trace_mu <- ggplot(data = posterior_data,
                            aes(x = i,
                                y = mu)) +
    geom_rect(
      aes(
        xmin = 1,
        xmax = len_warmup,
        ymin = mu_zoom_limits[1],
        ymax = mu_zoom_limits[2]
      ),
      fill = "gray80",
      alpha = 0.1,
      color = NA
    ) +
    geom_line(lwd = .3) +
    geom_smooth(col = "white",
                se = FALSE) +
    geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
    ylab("") +
    xlab("") +
    xlim(1, len_warmup + len_sample) +
    ylim(mu_zoom_limits) +
    rotate() +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))
  
  posterior_trace_sigma2 <- ggplot(data = posterior_data,
                                aes(x = i,
                                    y = sigma2)) +
    geom_rect(
      aes(
        xmin = 1,
        xmax = len_warmup,
        ymin = sigma2_zoom_limits[1],
        ymax = sigma2_zoom_limits[2]
      ),
      fill = "gray80",
      alpha = 0.1,
      color = NA
    ) +
    geom_line(lwd = .3) +
    geom_smooth(col = "white",
                se = FALSE) +
    geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
    ylab("") +
    xlab("") +
    xlim(1, len_warmup + len_sample) +
    ylim(sigma2_zoom_limits) +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))
  
  posterior_histogram_sigma2 <- posterior_data %>%
    ggplot(aes(x = sigma2)) +
    geom_histogram(bins = 50) +
    xlim(sigma2_zoom_limits) +
    rotate() +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))
  
  posterior_histogram_mu <- posterior_data %>%
    ggplot(aes(x = mu)) +
    geom_histogram(bins = 50) +
    xlim(mu_zoom_limits) +
    theme(
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))
  
  
  ggpubr::ggarrange(
    posterior_histogram_mu,
    NULL,
    NULL,
    posterior_trace_mu,
    NULL,
    NULL,
    posterior_density,
    posterior_trace_sigma2,
    posterior_histogram_sigma2,
    nrow = 3,
    ncol = 3,
    align = "hv",
    widths = c(2, 1, 1),
    heights = c(1, 1, 2)
  ) %>%
    annotate_figure(top = text_grob(paste0("After 50 warm-up + 1000 post-warmup draws")))
```

### Step 5: Discard the warmup draws

```{r posterior-post-warmup, fig.align='center', fig.width = 6, fig.height = 6}
posterior_data <- posterior_data %>%
  dplyr::mutate_at(
    .vars = vars(mu, sigma2),
    .funs = ~ifelse(i %in% seq_len(len_warmup), NA_real_, .)
  )

posterior_density <- posterior_data %>%
  ggplot(aes(x = mu, y = sigma2)) +
  stat_density_2d(aes(fill = ..density..),
                  geom = "raster",
                  contour = FALSE) +
  scale_fill_viridis_c() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  xlab(expression(mu)) +
  ylab(expression(sigma ^ 2))   +
  theme(legend.position = "none") +
  xlim(mu_zoom_limits) +
  ylim(sigma2_zoom_limits)

posterior_trace_mu <- ggplot(data = posterior_data,
                             aes(x = i,
                                 y = mu)) +
  geom_rect(
    aes(
      xmin = 1,
      xmax = len_warmup,
      ymin = mu_zoom_limits[1],
      ymax = mu_zoom_limits[2]
    ),
    fill = "gray80",
    alpha = 0.1,
    color = NA
  ) +
  geom_line(lwd = .3) +
  geom_smooth(col = "white",
              se = FALSE) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup + len_sample) +
  ylim(mu_zoom_limits) +
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

posterior_trace_sigma2 <- ggplot(data = posterior_data,
                                 aes(x = i,
                                     y = sigma2)) +
  geom_rect(
    aes(
      xmin = 1,
      xmax = len_warmup,
      ymin = sigma2_zoom_limits[1],
      ymax = sigma2_zoom_limits[2]
    ),
    fill = "gray80",
    alpha = 0.1,
    color = NA
  ) +
  geom_line(lwd = .3) +
  geom_smooth(col = "white",
              se = FALSE) +
  geom_vline(xintercept = len_warmup, color = "red", size = 0.5) +
  ylab("") +
  xlab("") +
  xlim(1, len_warmup + len_sample) +
  ylim(sigma2_zoom_limits) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

posterior_histogram_sigma2 <- posterior_data %>%
  ggplot(aes(x = sigma2)) +
  geom_histogram(bins = 50) +
  xlim(sigma2_zoom_limits) +
  rotate() +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))

posterior_histogram_mu <- posterior_data %>%
  ggplot(aes(x = mu)) +
  geom_histogram(bins = 50) +
  xlim(mu_zoom_limits) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  theme(plot.margin = unit(c(0, 0, 0, 0), 'lines'))


ggpubr::ggarrange(
  posterior_histogram_mu,
  NULL,
  NULL,
  posterior_trace_mu,
  NULL,
  NULL,
  posterior_density,
  posterior_trace_sigma2,
  posterior_histogram_sigma2,
  nrow = 3,
  ncol = 3,
  align = "hv",
  widths = c(2, 1, 1),
  heights = c(1, 1, 2)
) %>%
  annotate_figure(top = text_grob(paste0("1000 post-warmup draws")))

# Pre-process data for following steps
posterior_data <- posterior_data %>%
  filter(!(i %in% seq_len(len_warmup))) %>%
  dplyr::select(mu, sigma2)
```

### Step 6: Summarize the marginal posterior distributions via distributional summaries

#### Posterior medians as Bayesian analogues to point estimates

```{r posterior-medians, echo = TRUE}
round(apply(posterior_data, 2, median), 2)
```

#### Posterior standard deviations as Bayesian analogues to standard errors

```{r posterior-sds, echo = TRUE}
round(apply(posterior_data, 2, sd), 2)
```

#### Quantile-based credible intervals as Bayesian uncertainty intervals

```{r posterior-cis, echo = TRUE}
round(apply(posterior_data, 2, quantile, c(.025, .975)), 2)
```

#### Posterior proportions as Bayesian directional ("one-tailed") hypothesis tests

```{r posterior-probs, echo = TRUE}
mean(posterior_data$mu < 0)
mean(posterior_data$mu > -3.33)
```

## Convergence diagnostics

### Why diagnose?

MCMC algorithms use iterative algorithms to explore posterior
distributions and to produce numerical approximations thereof.

However, even with appropriately specified models and algorithms, we can
never know a priori if and when a chain has converged to its target
distribution. We must thus rely on *convergence diagnostics*.

*Important:* Convergence diagnostics cannot show or prove convergence.
They can only show signs of non-convergence!

### How to diagnose

To conclude that the post-warmup draws of our sampler in fact explore
the target distribution, we want to run *multiple, independent chains* on the
same data and show two things:

1.  Every chain is in a stationary state (i.e., does not "wander off"
    the target distribution) -- no trending *within-chain variance*
2.  All chains are in the same stationary state (i.e.,
    no convergence to different target distributions given identical
    data) -- no systematic *between-chain variance*

### Generic diagnostics

Generic diagnostics (see [Gill 2015, Ch.
14.3](https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483))
include:

1.  **Potential scale reduction statistic** $\hat{R}$ (a.k.a. Gelman-Rubin
    convergence diagnostic)
    $$\small \widehat{Var}(\theta) = (1 - \frac{1}{\mathtt{n_{iter}}})
     \underbrace{\Bigg(\frac{1}{ \mathtt{n_{chains}} (\mathtt{n_{iter}} - 1)} \sum_{j=1}^{\mathtt{n_{chains}}} \sum_{i=1}^{\mathtt{n_{iter}}} (\theta_{ij} - \bar{\theta_j})^2 \Bigg)}_{\text{Within chain var}} + 
     \frac{1}{\mathtt{n_{iter}}}  \underbrace{\Bigg(\frac{\mathtt{n_{iter}}}{\mathtt{n_{chains} - 1}} \sum_{j=1}^{\mathtt{n_{chains}}} (\bar{\theta_j} - \bar{\bar{\theta}})^2\Bigg)}_{\text{Between chain var}}$$
    -   low values indicate that chains are stationary (convergence to
        target distribution within chains)
    -   low values indicate that chains mix (convergence to same target
        distribution across chains)
2.  **Geweke Time-Series Diagnostic**: Compare non-overlapping
    post-warmup portions of each chain to test within-convergence
3.  **Heidelberger and Welch Diagnostic**: Compare early post-warmup
    portion of each chain with late portion to test within-convergence
4.  **Raftery and Lewis Integrated Diagnostic**: Evaluates the full
    chain of a pilot run (requires that `save_warmup = TRUE`) to
    estimate minimum required length of warmup and sampling

These are implemented as part of the `coda` package (Output Analysis and
Diagnostics for MCMC).

### Visual diagnostics

The most widespread visual diagnostics are:

1.  **Traceplots**: Visually inspect if chains are stationary and have
    converged to the same distribution
2.  **Autocorrelation plots**: Visually inspect if the chain is sluggish
    in exploring the parameter space.

### Application

In the following, we will use multiple chain runs of our sampler in
conjunction with the `coda` package to check for signs of
non-convergence.

Note that `coda` functions require that we combine our chains into
`mcmc.list` objects.

### Raftery and Lewis Integrated Diagnostic

The Raftery-Lewis diagnostic takes a single chain, including warm-up
draws, to estimate the minimum required length of warmup and sampling
runs:

```{r raftery-lewis, echo = TRUE}
# Example: Gill 2015, p. 503
# If we want a 95% credible interval around the median 
# with reliability between 92.5% and 97.5%, we need:
q <- 0.5    # quantile of interest
r <- 0.0125 # margin of error
s <- 0.95   # desired reliability

## The recommend length for the pilot run:
n <- ceiling((qnorm(.5 * (s + 1)) * sqrt(q * (1 - q)) / r) ^ 2)

# Pilot run
draws_pilot <-
  draw_from_posterior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_warmup = 0,
    n_draws = n,
    data = gles$sup_afd,
    keep_warmup = TRUE
  )

# Save as mcmc
draws_pilot <- as.mcmc(simplify2array(draws_pilot))

# Diagnose
raftery.diag(
  draws_pilot,
  q = q,
  r = r,
  s = s
)
```

### Gelman-Rubin, Geweke, and Heidelberger-Welch diagnostics

We will use the recommended run-length from the Raftery-Lewis diagnostic
for four independent runs of our sampler.

We will ensure that our chains run independently (i.e., using different
starting values and different random number sequences) by setting
different seed:

```{r multiple-chains, echo = T}
seeds <- sample(10001:99999, 4)
draws_multiple_chains <- lapply(seeds,
                                function(seed) {
                                  as.mcmc(simplify2array(
                                    draw_from_posterior(
                                      theta = 0,
                                      omega = .25,
                                      alpha = 2,
                                      beta = 10,
                                      n_warmup = 200,
                                      n_draws = 6147,
                                      data = gles$sup_afd,
                                      keep_warmup = FALSE,
                                      seed = seed
                                    )
                                  ))
                                })

# Save as mcmc.list
draws_multiple_chains <- as.mcmc.list(draws_multiple_chains)
```

```{r other-diagnostics, echo = TRUE}
# Diagnose
coda::gelman.diag(draws_multiple_chains, autoburnin = FALSE)
coda::geweke.diag(draws_multiple_chains, frac1 = .1, frac2 = .5)  
coda::heidel.diag(draws_multiple_chains, pvalue = .1)             
```

### Trace plots

```{r trace, echo = TRUE}
par(mfrow = c(1, 2))
coda::traceplot(draws_multiple_chains, smooth = TRUE)
```

### Autocorrelation plots

```{r autocorr, echo = TRUE}
coda::autocorr.plot(draws_multiple_chains)
```
