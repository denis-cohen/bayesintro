% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{float}
\floatplacement{figure}{ht}
\usepackage[section]{placeins}
\usepackage{longtable}
\usepackage{hyperref}
\hypersetup{colorlinks = true, linkcolor = blue, urlcolor = blue}
\widowpenalty10000
\clubpenalty10000
\usepackage[page,header]{appendix}
\usepackage{titletoc}
\usepackage{tocloft}
\usepackage{makecell}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Lecture: Applied Bayesian Statistics I},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Lecture: Applied Bayesian Statistics I}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\setstretch{1.5}
\hypertarget{reasons-for-going-bayesian}{%
\subsection{Reasons for going Bayesian}\label{reasons-for-going-bayesian}}

Practitioners choose to go Bayesian for a multitude of reasons. Any one
of the following or any combination of the following may apply (partly
based on Jackman 2009):

\hypertarget{paradigmatic-reasons-bayesian-inference-answers-the-question-one-really-asks}{%
\subsubsection{Paradigmatic reasons: Bayesian inference answers the question one really asks}\label{paradigmatic-reasons-bayesian-inference-answers-the-question-one-really-asks}}

Frequentist \(p\)-values tell you how likely it would be to find another
result as `strong' or `extreme' as the current result if the analysis at
hand would be repeated many times over.

It is therefore a statement about the plausibility of the \emph{data given a
null hypothesis}.

What we usually really want to make are statements about the
plausibility of a \emph{hypothesis given the data}.

\hypertarget{paradigmatic-reasons-bayesian-inference-is-accessible-and-intuitive}{%
\subsubsection{Paradigmatic reasons: Bayesian inference is accessible and intuitive}\label{paradigmatic-reasons-bayesian-inference-is-accessible-and-intuitive}}

Bayesian inference allows researchers to directly and intuitively
communicate results.

What researchers often really want to say is what is the probability of
an event, classification, or hypothesis in light of the data currently
at hand, e.g.:

\begin{itemize}
\tightlist
\item
  What is the probability that the treatment effect of an experiment
  is positive, negative, or greater than some threshold (e.g., given
  data from clinical pre-trials, what is the probability that a new
  vaccine reduces infections by at least 10\%?
\item
  What is the probability that a specific hypothesis is true?
\item
  What is the probability that an individual belongs to a certain
  latent class?
\end{itemize}

\hypertarget{paradigmatic-reasons-inference-beyond-nhst}{%
\subsubsection{Paradigmatic reasons: Inference beyond NHST}\label{paradigmatic-reasons-inference-beyond-nhst}}

Frequentist inference often goes hand in hand with null-hypothesis
significance testing (NHST), which involves binary statements about
statistical significance based on \(p\)-value thresholds.

This focus on binary significance has been widely criticized (e.g.,
\href{https://www.jstor.org/stable/449153}{Gill 1999}; \href{https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1527253}{McShane et al.,
2019}).

Bayesian inference via posterior distributions lends itself to
communicating support for specific hypothesis via gradual measures.

\hypertarget{logical-reasons}{%
\subsubsection{Logical reasons}\label{logical-reasons}}

Frequentist inference is based on data that can be repeatedly (and,
theoretically, infinitely often) sampled from an underlying population.
At face value, this requires a quasi-infinite stream of independently
and identically distributed data.

Given time and resources, this is plausible for some applications --
coin flips, physical or chemical experiments, or survey-based research
-- but not for others (e.g., a time-series analysis of annual imports
and economic productivity in the GDR).

Bayesian inference provides a principled framework for analyzing
non-repeatable data.

\hypertarget{substantive-reasons-incorporating-prior-beliefs}{%
\subsubsection{Substantive reasons: Incorporating prior beliefs}\label{substantive-reasons-incorporating-prior-beliefs}}

The necessity to specify prior beliefs is sometimes considered a
nuisance and/or weak point of Bayesian inference:

\begin{itemize}
\tightlist
\item
  In absence of prior beliefs, researchers typically specify
  uninformative and vague priors.
\item
  The inclusion of informative prior beliefs is often criticized as
  bringing ``subjectivity'' into the data analysis.
\end{itemize}

Yet researchers can often incorporate informative prior beliefs
productively.

Suppose you are on to a major contribution, based on a finding that runs
counter to conventional wisdom:

\begin{itemize}
\tightlist
\item
  You can accurately characterize prior knowledge based on a meta
  analysis of existing research.
\item
  You can then show that your analysis produces a deviating, novel
  result \emph{despite} the incorporation of contrary prior knowledge.
\end{itemize}

\hypertarget{pragmatic-reasons-stable-and-reliable-estimates}{%
\subsubsection{Pragmatic reasons: Stable and reliable estimates}\label{pragmatic-reasons-stable-and-reliable-estimates}}

Complex statistical models often fail to converge when using
likelihood-based estimators or are not at all implementable in existing
estimation frameworks.

Bayesian inference via numerical Markov Chain Monte Carlo methods
provide a powerful, versatile, and stable toolbox to overcome these
limitations.

\hypertarget{pragmatic-reasons-regularizing-priors}{%
\subsubsection{Pragmatic reasons: Regularizing priors}\label{pragmatic-reasons-regularizing-priors}}

Some important questions cannot be addressed with conventional
techniques due to overfitting.

Bayesian data analysis allows researchers to specify regularizing priors
that help prevent this problem.

\hypertarget{pragmatic-reasons-non-normal-posterior-distributions}{%
\subsubsection{Pragmatic reasons: Non-normal posterior distributions}\label{pragmatic-reasons-non-normal-posterior-distributions}}

Frequentist inference often assumes asymptotic normality of estimators
(i.e., that sampling distributions can be approximated by a normal
distribution in very large samples).

But samples aren't always that large and not all parameters plausibly
follow a normal distribution.

Bayesian inference gives a way out:

\begin{itemize}
\tightlist
\item
  Quantile summaries of posterior distributions instead of analytical,
  symmetrical confidence intervals
\item
  Direct specification of non-normal and/or constrained parameters
\end{itemize}

\hypertarget{trade-offs-priors}{%
\subsubsection{Trade-offs: Priors}\label{trade-offs-priors}}

\begin{itemize}
\tightlist
\item
  Choice of priors allows us to explicitly incorporate prior beliefs
  about parameters\ldots{}
\item
  \ldots but also comes with the obligation to be transparent and
  responsible with respect to the subjectivity this brings into our
  analyses
\end{itemize}

\hypertarget{trade-offs-finite-sample-and-asymptotic-properties}{%
\subsubsection{Trade-offs: Finite-sample and asymptotic properties}\label{trade-offs-finite-sample-and-asymptotic-properties}}

\begin{itemize}
\tightlist
\item
  Bayesian inference allows for exact inference in finite-sample
  applications where the asymptotic properties of MLE estimators are
  implausible (normal approximation, etc.)\ldots{}
\item
  \ldots yet, posterior distribution often asymptotically converge to the
  sampling distribution of MLE estimators (\href{https://en.wikipedia.org/wiki/Bernstein\%E2\%80\%93von_Mises_theorem}{Bernstein-von-Mises
  Theorem})
\end{itemize}

\hypertarget{trade-offs-flexibility-and-computational-reliability}{%
\subsubsection{Trade-offs: Flexibility and computational reliability}\label{trade-offs-flexibility-and-computational-reliability}}

\begin{itemize}
\tightlist
\item
  The use of MCMC algorithms for probabilistic approximate inference
  makes Bayesian approaches incredibly flexible and allows for
  computationally reliable estimation of complex, analytically
  intractable marginal likelihoods (avoids integration of super
  high-dimensional integrals)\ldots{}
\item
  \ldots but sometimes comes with the necessity of high computational
  resources and/or long computation times, and \emph{always} necessitates
  careful convergence diagnosis
\end{itemize}

\hypertarget{applied-bayesian-data-analysis-a-short-history}{%
\subsection{Applied Bayesian data analysis: A short history}\label{applied-bayesian-data-analysis-a-short-history}}

\hypertarget{classical-bayes-in-the-20th-century}{%
\subsubsection{Classical Bayes in the 20th century}\label{classical-bayes-in-the-20th-century}}

An important hallmark of the classical era of Bayesian statistics was the relative ease with which a joint posterior distribution that could not be integrated into the constituent marginal distributions could be produced with actual social science data and a realistic model based on theoretical principles.

It was easy in the middle of the 20th century to produce a model such as this whereby it was prohibitively difficult or impossible to integrate-out each parameter for a series of marginal posterior distributions to describe inferentially. This led Evans (1994) to retrospectively describe Bayesians of the time as `unmarried marriage guidance councilors' because they could tell others how to do inference when they often could not do it themselves.

\href{https://jeffgill.org/wp-content/uploads/2021/04/curini_franzese_v2_chp50_1pp_01.pdf}{Gill, J., \& Heuberger, S. (2020). Bayesian Modeling and Inference: A Postmodern Perspective. In L. Curini \& R. Franzese (Eds.), The SAGE Handbook of Research Methods in Political Science and International Relations (pp.~961--984).}

\hypertarget{the-revolution-computational-numerical-bayes-via-mcmc}{%
\subsubsection{The revolution: Computational numerical Bayes via MCMC}\label{the-revolution-computational-numerical-bayes-via-mcmc}}

This world, and \emph{the} world, changed in 1990 with a review essay by Gelfand and Smith in the \emph{Journal of the American Statistical Association}. In what is without a doubt one of the ten most important articles published in statistics, they found that a tool, Gibbs sampling, hiding in engineering and image restoration (Geman and Geman, 1984), solved this very problem for the Bayesians. Gibbs sampling replaced analytical derivation of marginals from a joint distribution with work from the computer.

The general name for the new tool is Markov chain Monte Carlo (MCMC), which includes Gibbs sampling {[}\ldots{]}. The principle behind MCMC is that a Markov chain can be setup to describe a high dimension posterior distribution by wandering around the state space visiting subregions in proportion to the density that needs to be summarized.

The linkage between `MC' and `MC' is the ergodic theorem that says that if the Markov chain is created according to some specific technical criteria and is run long enough such that it converges to its stationary (limiting) distribution, the draws from the path of the chain can be treated as if they are IID from the respective marginal distributions.

To say that MCMC revolutionized Bayesian inference and general Bayesian work is to say that commercial air flights slightly improved global travel. It was in fact the tool that freed the Bayesians; no longer were models too complex for the marginalization of joint posteriors to create regression tables and other intuitive summaries.

\href{https://jeffgill.org/wp-content/uploads/2021/04/curini_franzese_v2_chp50_1pp_01.pdf}{Gill, J., \& Heuberger, S. (2020). Bayesian Modeling and Inference: A Postmodern Perspective. In L. Curini \& R. Franzese (Eds.), The SAGE Handbook of Research Methods in Political Science and International Relations (pp.~961--984).}

\hypertarget{major-programming-language-and-software}{%
\subsubsection{Major programming language and software}\label{major-programming-language-and-software}}

The development and consolidation of free (and, later, free and open) software marked critical for the advancement of MCMC-based Bayesian data analysis. The most prominent such endeavors include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.mrc-bsu.cam.ac.uk/software/bugs/}{BUGS} (\textbf{B}ayesian Inference \textbf{U}sing \textbf{G}ibbs \textbf{S}ampling), a project started in the MRC Biostatistics Unit in Cambridge in 1989

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \href{https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/}{WinBUGS}, developed until 2007
  \item
    \href{https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/}{OpenBUGS}, 2005-2014
  \item
    \href{https://www.multibugs.org/}{MultiBUGS}, a parallelizable version of OpenBUGS, last updated 2020
  \end{enumerate}
\item
  \href{https://mcmc-jags.sourceforge.io/}{JAGS} (\textbf{J}ust \textbf{A}nother \textbf{G}ibbs \textbf{S}ampler)

  \begin{itemize}
  \tightlist
  \item
    A cousin of the BUGS langauge, written by \href{https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/plummer/}{Martyn Plummer} with the aim of providing a extensible cross-platform toolbox for Bayesian modeling
  \item
    Since 2007, latest release 2022
  \end{itemize}
\item
  \href{https://mc-stan.org/}{Stan}, a C++ based language and platform for statistical computation using Hamiltonian Monte Carlo algorithms

  \begin{itemize}
  \tightlist
  \item
    Cross-platform software that interfaces with many major statistical softwares, incl.~R, Python, shell, MATLAB, Julia, and Stata
  \item
    Initiated by the \href{https://mc-stan.org/about/team/}{Stan Development Team} (Gelman et al.) in 2012, ongoing
  \item
    Emphasis on fast and stable computation using state-of-the-art algorithms
  \end{itemize}
\end{enumerate}

\hypertarget{applied-bayesian-data-analysis-until-the-mid-2010s}{%
\subsubsection{Applied Bayesian data analysis until the mid-2010s}\label{applied-bayesian-data-analysis-until-the-mid-2010s}}

Despite significant advancements in MCMC-based Bayesian data analysis since the early 1990s, it arguably remained a niche methodology until the mid-2010s, albeit with an increasingly broadening niche.

Why? Because many steps in the data analytical workflow remained disproportionately cumbersome and idiosyncratic:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No toolbox for pre-processing

  \begin{itemize}
  \tightlist
  \item
    Software-specific manual pre-processing of data
  \item
    Software-specific manual implementation of model code (though textbooks and open science advocates contributed to an ever-growing collection of curated model codes)
  \item
    Software-specific manual initialization of MCMC algorithms
  \end{itemize}
\item
  Bayesian inference

  \begin{itemize}
  \tightlist
  \item
    Need to specify priors
  \item
    Inference in specific software (though many softwares interfaced with R early on)
  \item
    Computationally demanding and, therefore, time-consuming
  \end{itemize}
\item
  Need for convergence diagnosis
\item
  No toolbox for post-processing

  \begin{itemize}
  \tightlist
  \item
    Need to manually craft regression tables
  \item
    Need to manually calculate substantive quantities of interest (e.g., expected values, average marginal effects)
  \item
    Need to manually visualize quantities of interest
  \end{itemize}
\end{enumerate}

\hypertarget{example-i-a-linear-model-in-bugsjags}{%
\subsubsection{Example I: A linear model in BUGS/JAGS}\label{example-i-a-linear-model-in-bugsjags}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-}{-} Data {-}{-}{-}{-}}
\CommentTok{\# Load data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"some\_data.dta"}\NormalTok{, }\AttributeTok{convert.factors =}\NormalTok{ F)}

\CommentTok{\# Define data for JAGS}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(data)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3 }\SpecialCharTok{+}\NormalTok{ x4, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{K }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}

\CommentTok{\# Store all data we need for the model in a list}
\NormalTok{jags\_data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y,}
                  \AttributeTok{X =}\NormalTok{ X,}
                  \AttributeTok{K =}\NormalTok{ K,}
                  \AttributeTok{N =}\NormalTok{ N)}

\CommentTok{\# {-}{-}{-}{-} Linear Model {-}{-}{-}{-}}
\CommentTok{\# Define the model}
\NormalTok{model0 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
  \CommentTok{\# Likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
\NormalTok{    y[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(y.hat[i], tau)}
\NormalTok{    y.hat[i] }\OtherTok{\textless{}{-}} \FunctionTok{inprod}\NormalTok{(b[], X[i, ])}
    \CommentTok{\# equivalent: b1[i] + b2*x2[i] + b3*x3[i] + b4*x4[i]}
\NormalTok{  \}}
  
  \CommentTok{\# Priors}
\NormalTok{  tau }\OtherTok{\textless{}{-}} \FunctionTok{pow}\NormalTok{(sigma,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{) }\CommentTok{\# precision {-} inverse of variance}
\NormalTok{  sigma }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{K) \{}
\NormalTok{    b[k] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, .}\DecValTok{0001}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\# Save the model}
\FunctionTok{write.model}\NormalTok{(model0, }\StringTok{"model0.bug"}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-} Initial values (initial values as list of lists) {-}{-}{-}{-}}
\NormalTok{inits }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{b =} \FunctionTok{rnorm}\NormalTok{(K),}
    \AttributeTok{sigma =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \AttributeTok{.RNG.seed =} \DecValTok{123}\NormalTok{,}
    \AttributeTok{.RNG.name =} \StringTok{"base::Mersenne{-}Twister"}
\NormalTok{  ),}
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{b =} \FunctionTok{rnorm}\NormalTok{(K),}
    \AttributeTok{sigma =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \AttributeTok{.RNG.seed =} \DecValTok{456}\NormalTok{,}
    \AttributeTok{.RNG.name =} \StringTok{"base::Mersenne{-}Twister"}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-} Pass model to JAGS and perform inference {-}{-}{-}{-}}
\NormalTok{m\_0 }\OtherTok{\textless{}{-}} \FunctionTok{jags.model}\NormalTok{(}
  \AttributeTok{file =} \StringTok{"model0.bug"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ jags\_data,}
  \AttributeTok{n.chains =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{inits =}\NormalTok{ inits,}
  \AttributeTok{n.adapt =} \DecValTok{2000}
\NormalTok{)}

\CommentTok{\# Burn in}
\FunctionTok{update}\NormalTok{(m\_0, }\AttributeTok{n.iter =} \DecValTok{15000}\NormalTok{) }\CommentTok{\# 1st{-}15,000th iterations will be discarded}

\CommentTok{\# Run Sampler}
\NormalTok{m\_0\_out }\OtherTok{\textless{}{-}} \FunctionTok{coda.samples}\NormalTok{(}
  \AttributeTok{model =}\NormalTok{ m\_0,}
  \AttributeTok{variable.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"b"}\NormalTok{, }\StringTok{"sigma"}\NormalTok{),}
  \AttributeTok{n.iter =} \DecValTok{5000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{convenience-functionalities-for-r-users}{%
\subsubsection{Convenience functionalities for R users}\label{convenience-functionalities-for-r-users}}

\hypertarget{convenience-functionalities}{%
\paragraph{Convenience functionalities}\label{convenience-functionalities}}

\begin{itemize}
\tightlist
\item
  Printing of posterior summaries (regression tables) in the R console via \texttt{summary()}
\item
  Visual posterior summaries (histograms, densities, posterior medians with credible intervals) via \texttt{plot()}
\item
  Generic and visual diagnostics via the \texttt{coda} package
\end{itemize}

\hypertarget{outputs-users-had-to-craft-manually}{%
\paragraph{Outputs users had to craft manually}\label{outputs-users-had-to-craft-manually}}

\begin{itemize}
\tightlist
\item
  Formatted regression tables for Word/LaTeX/Markdown

  \begin{itemize}
  \tightlist
  \item
    Usually, a combination of posterior summaries + one of the \texttt{xtable}/\texttt{huxtable}/\texttt{kable}
  \item
    More recently, also convenience wrappers like \texttt{BayesPostEst}
  \end{itemize}
\item
  Substantive quantities of interest (incl., but not limited to, expected values, linear predictions, first differences, average marginal effects)
\item
  Visualizations of substantive quantities of interest
\end{itemize}

\hypertarget{example-a-linear-model-in-stan}{%
\subsubsection{Example: A linear model in Stan}\label{example-a-linear-model-in-stan}}

\hypertarget{data-pre-processing-r-same-as-for-bugsjags}{%
\paragraph{Data pre-processing (R) -- same as for BUGS/JAGS}\label{data-pre-processing-r-same-as-for-bugsjags}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-}{-} Data {-}{-}{-}{-}}
\CommentTok{\# Load data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"some\_data.dta"}\NormalTok{, }\AttributeTok{convert.factors =}\NormalTok{ F)}

\CommentTok{\# Define data for Stan}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(data)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3 }\SpecialCharTok{+}\NormalTok{ x4, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{K }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}

\CommentTok{\# Store all data we need for the model in a list}
\NormalTok{stan\_data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y,}
                  \AttributeTok{X =}\NormalTok{ X,}
                  \AttributeTok{K =}\NormalTok{ K,}
                  \AttributeTok{N =}\NormalTok{ N)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-code-stan}{%
\paragraph{Model code (Stan)}\label{model-code-stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} N;   }\CommentTok{// number of observations}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} K;   }\CommentTok{// number of predictors (incl. intercept)}
  \DataTypeTok{matrix}\NormalTok{[N, K] x;   }\CommentTok{// predictor matrix}
  \DataTypeTok{vector}\NormalTok{[N] y;      }\CommentTok{// outcome vector}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[K] beta;       }\CommentTok{// coefficients for intercept and predictors}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} sigma;  }\CommentTok{// error scale}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
 \CommentTok{// priors}
\NormalTok{  beta \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{);  }\CommentTok{// priors for beta}
\NormalTok{  sigma \textasciitilde{} cauchy(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);  }\CommentTok{// prior for sigma}

  \CommentTok{// likelihood}
\NormalTok{  y \textasciitilde{} normal(x * beta + alpha, sigma);}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{compilation-inference-and-summarize-r---c---r}{%
\paragraph{Compilation, inference, and summarize (R -\textgreater{} C++ -\textgreater{} R)}\label{compilation-inference-and-summarize-r---c---r}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compile the model in C++}
\NormalTok{stan\_lm }\OtherTok{\textless{}{-}} \FunctionTok{stan\_model}\NormalTok{(}\AttributeTok{file =} \StringTok{"lm.stan"}\NormalTok{)}

\CommentTok{\# Fit the model}
\NormalTok{est }\OtherTok{\textless{}{-}} \FunctionTok{sampling}\NormalTok{(stan\_lm,                           }\DocumentationTok{\#\#\# compiled model}
                \AttributeTok{data =}\NormalTok{ stan\_data,                  }\DocumentationTok{\#\#\# data input}
                \AttributeTok{algorithm =} \StringTok{"NUTS"}\NormalTok{,                }\DocumentationTok{\#\#\# algorithm}
                \AttributeTok{pars =} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"sigma"}\NormalTok{),         }\DocumentationTok{\#\#\# select parameters}
                \AttributeTok{iter =}\NormalTok{ 2000L,                      }\DocumentationTok{\#\#\# iter per chain}
                \AttributeTok{warmup =}\NormalTok{ 1000L,                    }\DocumentationTok{\#\#\# warmup period}
                \AttributeTok{chains =}\NormalTok{ 4L,                       }\DocumentationTok{\#\#\# num. chains}
                \AttributeTok{cores =}\NormalTok{ 4L,                        }\DocumentationTok{\#\#\# num. cores}
                \AttributeTok{seed =} \DecValTok{20190417}\NormalTok{)                   }\DocumentationTok{\#\#\# seed}

\CommentTok{\# Summarize}
\NormalTok{est}
\end{Highlighting}
\end{Shaded}

\hypertarget{convenience-functionalities-for-r-users-1}{%
\subsubsection{Convenience functionalities for R users}\label{convenience-functionalities-for-r-users-1}}

\hypertarget{convenience-functionalities-1}{%
\paragraph{Convenience functionalities}\label{convenience-functionalities-1}}

\begin{itemize}
\tightlist
\item
  Printing of posterior summaries (regression tables) and generic diagnostics in the R console via \texttt{print()}
\item
  Static and interactive visual posterior summaries via the \texttt{rstan}, \texttt{bayesplot} and \texttt{shinystan} packages
\item
  Static and interactive implementations of generic, algorithm-specific, and visual diagnostics via the \texttt{rstan}, \texttt{bayesplot} and \texttt{shinystan} packages
\end{itemize}

\hypertarget{outputs-users-had-to-craft-manually-1}{%
\paragraph{Outputs users had to craft manually}\label{outputs-users-had-to-craft-manually-1}}

\begin{itemize}
\tightlist
\item
  Formatted regression tables for Word/LaTeX/Markdown

  \begin{itemize}
  \tightlist
  \item
    Usually, a combination of posterior summaries + one of the \texttt{xtable}/\texttt{huxtable}/\texttt{kable}
  \item
    More recently, also convenience wrappers like \texttt{BayesPostEst}
  \end{itemize}
\item
  Substantive quantities of interest (incl., but not limited to, expected values, linear predictions, first differences, average marginal effects)
\item
  Visualizations of substantive quantities of interest
\end{itemize}

\hypertarget{applied-bayesian-data-analysis-since-the-mid-2010s}{%
\subsubsection{Applied Bayesian data analysis since the mid-2010s}\label{applied-bayesian-data-analysis-since-the-mid-2010s}}

Over the last decade, applied Bayesian modeling has evolved from a niche methodology with high computational and software-specific entry barriers to a readily available toolbox that virtually everyone can use by running pre-implemented packages in standard statistical software on generic PCs.

What happened? In my view:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Stan \emph{really} took off: Its modern approach to software presentation and software development attracted users and developers alike.
\item
  The speed of Stan's algorithm, coupled with increasing RAM and CPU performances of home computers and the increasing availability of remote computing services, made computational demands a much smaller obstacle.
\item
  The Stan Development Team and dedicated members of the Stan Community (some of whom later joined the Development Team) put significant efforts into developing flexible, versatile, reliable, and easy-to-use softwares interfacing Stan that allow \textbf{R} users to run Bayesian analyses:

  \begin{itemize}
  \tightlist
  \item
    without software-specific manual pre-processing of data
  \item
    without software-specific manual implementation of model code
  \item
    without software-specific manual initialization of MCMC algorithms
  \item
    without needing to make informed choices about priors due to defaults (\emph{note:} not everybody is thrilled about that.)
  \item
    with access to a large number of model types
  \item
    with accessible, intuitive, and interactive tools for convergence diagnosis
  \item
    with convenience functions for post-processing
  \end{itemize}
\item
  In light of the increasing popularity of Stan, developers of general statistical convenience software for \textbf{R} extended the compatibility of their software to convenience softwares interfacing \textbf{Stan} (e.g., \texttt{emmeans}, \texttt{marginaleffects}, \texttt{brmsmargins})
\end{enumerate}

\hypertarget{stan}{%
\subsection{Stan}\label{stan}}

\hypertarget{why-stan}{%
\subsubsection{Why Stan?}\label{why-stan}}

\begin{itemize}
\tightlist
\item
  Open-source software
\item
  Fast and stable algorithms
\item
  High flexibility with few limitations
\item
  Extensive documentation

  \begin{itemize}
  \tightlist
  \item
    \href{https://mc-stan.org/docs/2_19/stan-users-guide/index.html}{User's Guide}
  \item
    \href{https://mc-stan.org/docs/2_19/reference-manual/index.html}{Language Reference Manual}
  \item
    \href{https://mc-stan.org/docs/2_19/functions-reference/index.html}{Language Functions Reference}
  \end{itemize}
\item
  Highly transparent development process; see \href{https://github.com/stan-dev/stan}{Stan Development Repository on Github}
\item
  Very responsive \href{https://mc-stan.org/about/team/}{Development Team}
\item
  Large and active community in the \href{https://discourse.mc-stan.org/}{Stan Forums} and \href{https://stackoverflow.com/questions/tagged/stan}{Stack OVerflow}
\item
  Increasing number of \href{https://mc-stan.org/users/documentation/case-studies.html}{case studies} on specific classes of statistical models and specific aspects of using Stan, \href{https://mc-stan.org/users/documentation/tutorials.html}{tutorials} about Stan and HMC, specialized field guides for using Stan for \href{https://education-stan.github.io/}{education research}, \href{https://stanecology.github.io/}{ecology}, \href{https://epidemiology-stan.github.io/}{epidemiology}, and \href{https://cognitive-science-stan.github.io/}{cognitive science}, as well as an increasing number \href{https://mc-stan.org/users/documentation/external.html}{papers and textbooks}
\item
  Compatibility with various editor for syntax highlighting, formatting, and checking (incl.~\href{https://www.rstudio.com/}{RStudio} and \href{https://www.gnu.org/software/emacs/}{Emacs})
\end{itemize}

\hypertarget{stan-interfaces}{%
\subsubsection{Stan interfaces}\label{stan-interfaces}}

\begin{itemize}
\tightlist
\item
  RStan (R)
\item
  PyStan (Python)
\item
  CmdStan (shell, command-line terminal)

  \begin{itemize}
  \tightlist
  \item
    CmdStanR
  \item
    CmdStanPy
  \end{itemize}
\item
  MatlabStan (MATLAB)
\item
  Stan.jl (Julia)
\item
  StataStan (Stata)
\item
  MathematicaStan (Mathematica)
\item
  ScalaStan (Scala)
\end{itemize}

\hypertarget{some-r-packages}{%
\subsubsection{(Some) R packages}\label{some-r-packages}}

\begin{itemize}
\tightlist
\item
  \href{https://cran.r-project.org/package=rstan}{\textbf{rstan}}: General R Interface to Stan
\item
  \href{https://cran.r-project.org/package=brms}{\textbf{brms}}: Bayesian Regression Models using `Stan', covering a growing number of model types
\item
  \href{https://cran.r-project.org/package=rstanarm}{\textbf{rstanarm}}: Bayesian Applied Regression Modeling via Stan, with an emphasis on hierarchical/multilevel models
\item
  \href{https://www.rdocumentation.org/packages/rethinking/versions/2.13}{\textbf{rethinking}}: An R package that accompanies McElreath's course and book \emph{Statistical Rethinking}, featuring functionalities for statistical modeling
\item
  \href{https://cran.r-project.org/package=edstan}{\textbf{edstan}}: Stan models for item response theory
\item
  \href{https://cran.r-project.org/web/packages/blavaan/index.html}{\textbf{blavaan}}: Bayesian latent variable analysis and structural equation modeling
\item
  \href{https://cran.r-project.org/package=shinystan}{\textbf{shinystan}}: Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models
\item
  \href{https://cran.r-project.org/web/packages/bayesplot/index.html}{\textbf{bayesplot}}: Plotting functions for posterior analysis, model checking, and MCMC diagnostics.
\item
  \href{https://cran.r-project.org/web/packages/shinystan/index.html}{\textbf{shinystan}}: Interactive visual and numerical diagnostics and posterior analysis for Bayesian models
\item
  \href{https://cran.r-project.org/package=rstantools}{\textbf{rstantools}}: Tools for Developing R Packages Interfacing with `Stan'
\item
  \href{https://cran.r-project.org/web/packages/loo/index.html}{\textbf{loo}}: Efficient leave-one-out cross-validation and WAIC for Bayesian models
\end{itemize}

\hypertarget{the-benefit-of-the-stan-package-landscape-in-r}{%
\subsubsection{The benefit of the Stan package landscape in R}\label{the-benefit-of-the-stan-package-landscape-in-r}}

Today, Stan offers both

\begin{itemize}
\tightlist
\item
  a high-end platform for model development and model building for advanced users in search of custom modeling solutions (\(\rightarrow\) \emph{Advanced Bayesian Statistical Modeling} -- Stan and \texttt{rstan})
\item
  a versatile and relatively easy-to-use toolbox for applied researchers using \textbf{R} who seek to go Bayesian using pre-implemented models and functions (\(\rightarrow\) \emph{Introduction to Bayesian Statistics} -- \texttt{brms}; also \texttt{rstanarm}, \texttt{rethinking})
\end{itemize}

\hypertarget{a-simplified-bayesian-workflow}{%
\subsection{A simplified Bayesian workflow}\label{a-simplified-bayesian-workflow}}

\hypertarget{the-very-short-version}{%
\subsubsection{The very short version}\label{the-very-short-version}}

\emph{Bayesian workflow} includes the three steps of model building, inference, and model checking/improvement, along with the comparison of different models, not just for the purpose of model choice or model averaging but more importantly to better understand these models.

Source: \href{https://arxiv.org/abs/2011.01808}{Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P. C., \& Modrák, M. (2020). Bayesian workflow.}

\hypertarget{the-long-version}{%
\subsubsection{The long version}\label{the-long-version}}

\begin{center}\includegraphics[width=0.9\linewidth]{images/workflow} \end{center}

Source: \href{https://arxiv.org/abs/2011.01808}{Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P. C., \& Modrák, M. (2020). Bayesian workflow.}

\hypertarget{a-moderately-long-version}{%
\subsubsection{A moderately long version}\label{a-moderately-long-version}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Specification}: Specify the full probability model

  \begin{itemize}
  \tightlist
  \item
    data
  \item
    likelihood
  \item
    priors
  \end{itemize}
\item
  \textbf{Model Building}: Translate the model into code
\item
  \textbf{Validation}: Validate the model with fake data
\item
  \textbf{Fitting}: Fit the model to actual data
\item
  \textbf{Diagnosis}: Check generic and algorithm-specific diagnostics to assess convergence
\item
  Posterior Predictive Checks
\item
  Model Comparison
\end{enumerate}

Source: \href{http://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/IntroToStan_basics_workflow.ipynb}{Jim Savage (2016) A quick-start introduction to Stan for economists. A QuantEcon Notebook.}

\hypertarget{foregoing-model-building-simplifies-the-workflow}{%
\subsubsection{Foregoing model building simplifies the workflow}\label{foregoing-model-building-simplifies-the-workflow}}

Given we do not need to build our own \emph{model code} from scratch, our workflow simplifies quite a bit.

But we will need to build our models in the sense that we set the \emph{model specification}.

Therefore, many steps of the workflow remain in the hand of us as applied researchers. Many of these steps pertain to frequentist and Bayesian analysis alike. How relevant and challenging each of these steps is likely depends on the specific application at hand.

The below is a (likely incomplete) list of steps we may want to take as part of a principled simplified workflow using pre-implemented models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initial model choice
\item
  Prior choice and prior predictive checks
\item
  Convergence diagnosis
\item
  Modification, expansion, or simplification of the model
\item
  Algorithmic fine-tuning
\item
  Posterior predictive checks
\item
  Cross-validation (out-of-sample-prediction)
\item
  Model comparison
\end{enumerate}

\hypertarget{further-reading}{%
\subsubsection{Further reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/2011.01808}{Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P. C., \& Modrák, M. (2020). \emph{Bayesian workflow.} ArXiv.}
\item
  \href{https://vasishth.github.io/bayescogsci/book/ch-workflow.html}{- Nicenboim, B., Schad, D., \& Vasishth, S. (2023). \emph{An Introduction to Bayesian Data Analysis for Cognitive Science.} Ch. 7.}
\end{itemize}

\end{document}
